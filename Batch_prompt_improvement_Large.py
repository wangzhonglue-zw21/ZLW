# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15QkLpJU9viJ_s0OKNghn8j9WqEdmauxl
"""

import torch
import re
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset

class DualModelBatchOptimizer:
    def __init__(self,
                 manager_model="Qwen/Qwen2.5-7B-Instruct",
                 worker_model="Qwen/Qwen2.5-Math-7B-Instruct"):

        self.manager_name = manager_model
        self.worker_name = worker_model
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # We process in phases to save VRAM (Load Manager -> Unload -> Load Worker -> Unload -> Load Manager)
        self.model = None
        self.tokenizer = None
        self.current_model_type = None

        # State
        self.current_system_prompt = self._get_initial_prompt()
        self.batch_history = []

    def _get_initial_prompt(self):
        return """You are a mathematical hardness analyst. Your goal is to predict the token budget for a solver model.

[CONTEXT INFO]
Problems in this specific problem set typically require between **200 and 2000 tokens** to solve.

[PROBLEM]
{problem_text}

Step 1: **Categorization and Structural Analysis**
First, classify the problem. Identify recurring mathematical forms.

Step 2: **Mental Simulation**
Break down the problem into its main components verbally.

Step 3: **Complexity Assessment**
Rate each parameter (1-10 scale):
- Conceptual Depth
- Computational Effort
- Abstraction Level
- Heuristic Requirements
- Constrainedness

Step 4: **Final Synthesis and Budget Output**
Based on this comprehensive analysis, output the final token budget.

Output your entire thought process followed by:
[TOKENS] <Integer>"""

    # --- MODEL MANAGEMENT (VRAM SAVER) ---
    def _load_model(self, model_name):
        if self.model is not None:
            del self.model
            del self.tokenizer
            gc.collect()
            torch.cuda.empty_cache()

        print(f"üöÄ Loading {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto" # or self.device
        )

    def _generate(self, prompt, max_tokens=2048):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                do_sample=True
            )
        return self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()

    # --- PHASE 1: MANAGER PREDICTIONS ---
    def run_manager_batch(self, problems):
        """Run the Manager model to analyze and predict tokens for the batch."""
        print(f"\nüîµ PHASE 1: MANAGER PREDICTING ({len(problems)} problems)...")
        if self.current_model_type != "manager":
            self._load_model(self.manager_name)
            self.current_model_type = "manager"

        predictions = []
        for i, problem in enumerate(problems):
            print(f"   -> Analyzing Problem {i+1}/{len(problems)}")

            # Construct Prompt
            if "{problem_text}" in self.current_system_prompt:
                prompt_text = self.current_system_prompt.replace("{problem_text}", problem)
            else:
                prompt_text = self.current_system_prompt + f"\n\n[PROBLEM]\n{problem}"

            # Generate
            response = self._generate(prompt_text, max_tokens=1024)

            # Extract Tokens
            match = re.search(r"\[TOKENS\]\s*(\d+)", response)
            predicted_val = int(match.group(1)) if match else 0

            # Clean reasoning (remove the token tag for storage)
            clean_reasoning = response[:match.start()] if match else response

            predictions.append({
                "problem": problem,
                "manager_reasoning": clean_reasoning,
                "predicted_tokens": predicted_val
            })

        return predictions

    # --- PHASE 2: WORKER SOLUTIONS ---
    def run_worker_batch(self, prediction_data):
        """Run the Worker model to solve the problems and get ground truth."""
        print(f"\nü§ñ PHASE 2: WORKER SOLVING ({len(prediction_data)} problems)...")
        if self.current_model_type != "worker":
            self._load_model(self.worker_name)
            self.current_model_type = "worker"

        completed_batch = []
        for i, data in enumerate(prediction_data):
            print(f"   -> Solving Problem {i+1}/{len(prediction_data)}")

            problem = data["problem"]
            worker_prompt = (
                "You are a highly capable math solver. Solve the following problem step-by-step.\n"
                "Put your final answer in \\boxed{}.\n\n"
                f"Problem: {problem}\n\nSolution:"
            )

            solution = self._generate(worker_prompt, max_tokens=2048)

            # Calculate Actual Token Cost (approx)
            actual_tokens = len(self.tokenizer.encode(solution))

            # Combine Data
            entry = data.copy()
            entry["worker_solution"] = solution
            entry["actual_tokens"] = actual_tokens
            entry["error"] = abs(entry["predicted_tokens"] - actual_tokens)

            completed_batch.append(entry)

        return completed_batch

    # --- PHASE 3: REFLECTION & OPTIMIZATION ---
    def reflect_and_optimize(self, batch_data):
        """
        1. Summarize each case (Problem, Analysis, Solution).
        2. Analyze deficiencies across the whole batch.
        3. Write a NEW SYSTEM PROMPT.
        """
        print(f"\nüß¨ PHASE 3: DEEP REFLECTION & OPTIMIZATION...")

        # Switch back to Manager (who is also the Optimizer)
        if self.current_model_type != "manager":
            self._load_model(self.manager_name)
            self.current_model_type = "manager"

        # --- STEP 3A: SUMMARIZE THE CASES ---
        print("   -> Generating Case Summaries...")

        # We will format the batch data into a readable block for the LLM
        case_studies_text = ""
        for i, item in enumerate(batch_data):
            case_studies_text += f"""
=== CASE {i+1} ===
[PROBLEM]
{item['problem']}

[MANAGER ANALYSIS (PREDICTION: {item['predicted_tokens']})]
{item['manager_reasoning']}

[WORKER SOLUTION (ACTUAL: {item['actual_tokens']})]
{item['worker_solution'][:800]}... (truncated for brevity)
==================
"""

        # Prompt for Summarization
        summary_prompt = f"""You are a Lead Researcher analyzing AI performance logs.
Here are {len(batch_data)} cases where a 'Manager' model tried to predict the difficulty (token count) of a math problem, and a 'Worker' model solved it.

{case_studies_text}

**TASK:**
For EACH case, write a concise summary (3-4 sentences) that covers:
1. **Problem Core:** What is the math domain (e.g., Number Theory, Geometry) and specific difficulty?
2. **Analysis Capture:** Did the Manager's analysis correctly identify the hard parts?
3. **Solution Reality:** What specific technique did the Worker use that drove the token cost (e.g., extensive case checking, simple algebra)?

Output format:
**Case 1:** [Summary]
**Case 2:** [Summary]
...
"""
        summaries = self._generate(summary_prompt, max_tokens=1500)
        print("\n[GENERATED SUMMARIES]")
        print(summaries)

        # --- STEP 3B: GENERATE NEW PROMPT ---
        print("\n   -> Synthesizing New System Prompt...")

        optimization_prompt = f"""
You are an Expert Prompt Engineer. You are optimizing a "Token Prediction System Prompt".
Review the Case Summaries below derived from a recent test batch.

[CASE SUMMARIES]
{summaries}

[CURRENT SYSTEM PROMPT]
{self.current_system_prompt}

[GOAL]
The current prompt failed to accurately predict token usage in several cases.
Your job is to write a **NEW SYSTEM PROMPT** that fixes these deficiencies.

[RULES & RESTRICTIONS]
1. **Analyze First:** Start by listing the top 3 patterns of failure (e.g., "Underestimated Geometry construction costs", "Overestimated simple arithmetic").
2. **Methodology:** Use the '5-Point Protocol':
   - Shift from Keyword Matching to **Solution Archetypes** (e.g. Constructive Proof vs Calculation).
   - Demand **Narrative Simulation** (Visualizing the steps).
   - Add **Volume Multipliers** for specific triggers (e.g., "If Case Work -> Multiply estimate by 1.5").
3. **Format Discipline:** The NEW prompt MUST end with the exact output format: "Output your entire thought process followed by: [TOKENS] <Integer>".
4. **Placeholder:** You MUST include `{{problem_text}}` in the new prompt.

[OUTPUT FORMAT]
### ANALYSIS
(Your analysis of the failures)

### NEW SYSTEM PROMPT
(The full, ready-to-use prompt text)
"""

        result = self._generate(optimization_prompt, max_tokens=2000)
        return result

# --- EXECUTION PIPELINE ---
def main():
    # 1. Init
    optimizer = DualModelBatchOptimizer(
        manager_model="Qwen/Qwen2.5-7B-Instruct",
        worker_model="Qwen/Qwen2.5-Math-7B-Instruct"
    )

    # 2. Data Loading
    print("üìö Loading Dataset (NuminaMath-CoT)...")
    dataset = load_dataset("AI-MO/NuminaMath-CoT", split="train")

    # Select 10 diverse problems (skipping the very first ones which might be trivial)
    batch_indices = range(200, 210)
    problems = [dataset[i]['problem'] for i in batch_indices]

    # 3. Run Batch Cycle
    # Step A: Manager predicts
    prediction_data = optimizer.run_manager_batch(problems)

    # Step B: Worker solves (Ground Truth)
    # Note: This automatically unloads Manager and loads Worker to save memory
    full_batch_data = optimizer.run_worker_batch(prediction_data)

    # 4. Display Stats
    print("\nüìä BATCH STATISTICS:")
    total_error = sum(d['error'] for d in full_batch_data)
    print(f"   Mean Absolute Error: {total_error / len(full_batch_data):.2f} tokens")
    for d in full_batch_data:
        print(f"   - Pred: {d['predicted_tokens']} | Actual: {d['actual_tokens']} | Diff: {d['error']}")

    # 5. Reflection (The "Smart" Part)
    # Note: This automatically unloads Worker and loads Manager
    optimization_result = optimizer.reflect_and_optimize(full_batch_data)

    print("\n" + "="*50)
    print("FINAL OPTIMIZATION RESULT")
    print("="*50)
    print(optimization_result)

    # Extract the new prompt if needed for saving
    if "### NEW SYSTEM PROMPT" in optimization_result:
        new_prompt = optimization_result.split("### NEW SYSTEM PROMPT")[1].strip()
        print("\n‚úÖ New prompt extracted and ready for next iteration.")

if __name__ == "__main__":
    main()

# Run this in a SEPARATE CELL - it will reuse the loaded dataset and models
# Make sure the main script has run first so 'dataset' exists in memory

print("="*60)
print("üß™ QUICK OPTIMIZED PROMPT TEST")
print("="*60)

# Check if dataset exists from previous run
try:
    test_problems = [dataset[i]['problem'] for i in range(300, 305)]  # Just 5 problems for quick test
    print("‚úÖ Dataset found in memory")
except NameError:
    print("‚ùå Dataset not found. Please run the main script first, or load dataset here:")
    from datasets import load_dataset
    dataset = load_dataset("AI-MO/NuminaMath-CoT", split="train")

# Import what we need
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

# The optimized prompt
optimized_prompt = """You are a mathematical hardness analyst. Your goal is to predict the token budget for a solver model.

[CONTEXT INFO]
Problems in this specific problem set typically require between **200 and 2000 tokens** to solve.

[PROBLEM]
{problem_text}

### Step 1: **Categorization and Structural Analysis**
Identify the primary mathematical form and any recurring patterns in the problem. Consider the following archetypes:
- **Algebraic Manipulation**: Problems involving detailed algebraic transformations.
- **Calculus and Analysis**: Problems requiring derivatives, integrals, or limits.
- **Trigonometry and Number Theory**: Problems involving trigonometric functions, counting principles, or number theoretic concepts.
- **Geometry and Analytic Geometry**: Problems involving geometric shapes, coordinate systems, or spatial relationships.
- **Logic and Algorithm Design**: Problems involving logical statements, algorithm validation, or computational complexity.
- **Sequences and Series**: Problems involving arithmetic or geometric sequences, subset sums, or recursive definitions.

### Step 2: **Mental Simulation**
Break down the problem into its main components and visualize the steps required to solve it. For example, if the problem involves detailed algebraic manipulation, imagine the steps involved in simplifying expressions, solving equations, or factoring polynomials.

### Step 3: **Complexity Assessment**
Rate each parameter (1-10 scale):
- **Conceptual Depth**: How complex is the underlying mathematical concept?
- **Computational Effort**: How many detailed steps are required to solve the problem?
- **Abstraction Level**: How abstract is the problem? (e.g., working with general functions vs. specific numerical examples)
- **Heuristic Requirements**: Are there specific heuristics or strategies needed to approach the problem?
- **Constrainedness**: How constrained is the problem? (e.g., bounded intervals, specific domain restrictions)

### Step 4: **Volume Multipliers**
Apply volume multipliers based on the problem type:
- **Algebraic Manipulation**: If the problem involves detailed algebraic manipulation, multiply the estimated token count by 1.5.
- **Sequences and Series**: If the problem involves sequences and series, multiply the estimated token count by 1.2.
- **Case Work**: If the problem requires case work, multiply the estimated token count by 1.5.
- **Special Cases**: If the problem has special cases or edge conditions, add an additional 10% to the estimated token count.

### Step 5: **Final Synthesis and Budget Output**
Based on this comprehensive analysis, output the final token budget.

Output your entire thought process followed by:
[TOKENS] <Integer>"""

# Quick generation function
def quick_generate(model, tokenizer, prompt, max_tokens=1024):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=0.7,
            do_sample=True
        )
    return tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()

# Load models (check if already loaded)
manager_name = "Qwen/Qwen2.5-7B-Instruct"
worker_name = "Qwen/Qwen2.5-Math-7B-Instruct"

print("\nüöÄ Loading Manager model...")
manager_tokenizer = AutoTokenizer.from_pretrained(manager_name)
manager_model = AutoModelForCausalLM.from_pretrained(
    manager_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# Test problems
test_indices = range(300, 305)  # Just 5 for quick test
test_problems = [dataset[i]['problem'] for i in test_indices]

print(f"\nüîµ Running predictions on {len(test_problems)} problems...")
predictions = []

for i, problem in enumerate(test_problems):
    print(f"   -> Problem {i+1}/{len(test_problems)}")
    prompt = optimized_prompt.replace("{problem_text}", problem)
    response = quick_generate(manager_model, manager_tokenizer, prompt)

    match = re.search(r"\[TOKENS\]\s*(\d+)", response)
    pred_tokens = int(match.group(1)) if match else 0

    predictions.append({
        "problem": problem,
        "predicted": pred_tokens
    })

# Get ground truth
print("\nüöÄ Loading Worker model...")
del manager_model
del manager_tokenizer
torch.cuda.empty_cache()

worker_tokenizer = AutoTokenizer.from_pretrained(worker_name)
worker_model = AutoModelForCausalLM.from_pretrained(
    worker_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

print(f"\nü§ñ Getting ground truth for {len(predictions)} problems...")
results = []

for i, pred in enumerate(predictions):
    print(f"   -> Solving {i+1}/{len(predictions)}")
    worker_prompt = f"You are a highly capable math solver. Solve the following problem step-by-step.\nPut your final answer in \\boxed{{}}.\n\nProblem: {pred['problem']}\n\nSolution:"

    solution = quick_generate(worker_model, worker_tokenizer, worker_prompt, max_tokens=2048)
    actual = len(worker_tokenizer.encode(solution))
    error = abs(pred['predicted'] - actual)

    results.append({
        "predicted": pred['predicted'],
        "actual": actual,
        "error": error,
        "error_pct": (error / actual * 100) if actual > 0 else 0
    })

# Results
print("\n" + "="*60)
print("üìä RESULTS")
print("="*60)

mae = sum(r['error'] for r in results) / len(results)
baseline = 480.60

print(f"\nüìà Performance:")
print(f"   Baseline MAE:  {baseline:.2f} tokens")
print(f"   Optimized MAE: {mae:.2f} tokens")

if mae < baseline:
    print(f"   ‚úÖ Improvement: {baseline - mae:.2f} tokens ({(baseline-mae)/baseline*100:.1f}%)")
else:
    print(f"   ‚ö†Ô∏è  Change: +{mae - baseline:.2f} tokens")

print(f"\nüìã Details:")
for i, r in enumerate(results, 1):
    print(f"   {i}. Pred: {r['predicted']:4d} | Actual: {r['actual']:4d} | Error: {r['error']:4d} ({r['error_pct']:.1f}%)")

print("\n‚úÖ Test complete!")