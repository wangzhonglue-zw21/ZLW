# -*- coding: utf-8 -*-
"""saikiranreddy

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skSJh0AMlobm3d-r96ueUJwknJmV1kPu
"""

import torch
import re
import gc
import json
import numpy as np
from collections import deque
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Any
from enum import Enum
from transformers import AutoTokenizer, AutoModelForCausalLM, logging as hf_logging
from datasets import load_dataset
import time
import math

# ===== CONFIGURATION =====
class Config:
    """Centralized configuration for the Token Budgeting System."""
    # Model Configuration
    MANAGER_MODEL = "Qwen/Qwen2.5-7B-Instruct"
    WORKER_MODEL = "Qwen/Qwen2.5-Math-7B-Instruct"
    DTYPE = torch.bfloat16

    DEVICE_MAP = "auto"

    # Dataset Configuration
    DATASET_NAME = "AI-MO/NuminaMath-CoT"
    START_INDEX = 800
    NUM_PROBLEMS = 50

    # Token Budget Limits
    MIN_TOKENS = 150
    MAX_TOKENS = 4096
    DEFAULT_TOKENS = 500

    # Confidence Thresholds
    HIGH_CONFIDENCE_THRESHOLD = 0.8
    MEDIUM_CONFIDENCE_THRESHOLD = 0.5
    LOW_CONFIDENCE_THRESHOLD = 0.3

    # Retry Configuration
    MAX_RETRIES = 2
    RETRY_MULTIPLIER_TIMEOUT = 2.0
    RETRY_MULTIPLIER_WRONG = 1.5

    # Learning Parameters
    EMA_ALPHA = 0.3
    HISTORY_WINDOW = 20

    # Penalty Weights
    UNDERPREDICTION_PENALTY = 2.0
    OVERPREDICTION_PENALTY = 1.0
    EXCELLENT_REWARD = -0.2

hf_logging.set_verbosity_error()


# ==========================================
# STRUCTURES
# ==========================================
class DifficultyLevel(Enum):
    """Problem difficulty classification."""
    TRIVIAL = 1
    EASY = 2
    MEDIUM = 3
    HARD = 4
    VERY_HARD = 5
    EXTREME = 6


@dataclass
class ProblemFeatures:
    """Extracted features from a problem."""
    char_count: int = 0
    word_count: int = 0
    equation_count: int = 0
    variable_count: int = 0
    number_count: int = 0
    parentheses_depth: int = 0
    fraction_count: int = 0
    part_count: int = 0
    question_count: int = 0
    complexity_keywords: int = 0
    case_indicators: int = 0
    step_indicators: int = 0
    category: str = "unknown"

    def to_dict(self) -> Dict:
        return {
            'char_count': self.char_count,
            'word_count': self.word_count,
            'equation_count': self.equation_count,
            'variable_count': self.variable_count,
            'number_count': self.number_count,
            'parentheses_depth': self.parentheses_depth,
            'fraction_count': self.fraction_count,
            'part_count': self.part_count,
            'question_count': self.question_count,
            'complexity_keywords': self.complexity_keywords,
            'case_indicators': self.case_indicators,
            'step_indicators': self.step_indicators,
            'category': self.category
        }


@dataclass
class ConfidenceScore:
    """Dual metric confidence assessment result."""
    self_reported: float = 0.5      # Model's own confidence
    peer_review: float = 0.5        # Historical/cross-validation confidence
    combined: float = 0.5           # Weighted combination
    method: str = "default"         # How confidence was calculated
    details: Dict = field(default_factory=dict)

    def __post_init__(self):
        # Ensure combined is calculated
        if self.combined == 0.5 and (self.self_reported != 0.5 or self.peer_review != 0.5):
            self.combined = 0.6 * self.self_reported + 0.4 * self.peer_review


@dataclass
class TokenAllocation:
    """Token allocation decision."""
    base_prediction: int = 500
    confidence_adjusted: int = 500
    safety_factor: float = 1.3
    final_budget: int = 650
    difficulty_level: DifficultyLevel = DifficultyLevel.MEDIUM
    allocation_reason: str = ""


@dataclass
class ExecutionResult:
    """Result of problem execution."""
    success: bool = False
    predicted_tokens: int = 0
    actual_tokens: int = 0
    budget_used: int = 0
    hit_limit: bool = False
    retries: int = 0
    model_answer: str = ""
    true_answer: str = ""
    penalty: float = 0.0
    penalty_type: str = ""
    execution_time: float = 0.0
    confidence: ConfidenceScore = field(default_factory=ConfidenceScore)


@dataclass
class SequenceState:
    """State for sequential problem handling."""
    problem_idx: int = 0
    cumulative_success: int = 0
    cumulative_failures: int = 0
    consecutive_failures: int = 0
    tokens_saved: int = 0
    tokens_wasted: int = 0
    cascade_risk: float = 0.0


# ==========================================
#  MODULE 1: DIFFICULTY ASSESSMENT
# ==========================================
class DifficultyAssessor:
    """
    Assesses problem difficulty using multiple signals.
    Maps to proposal: "Levels of difficulty... are hard to quantify"
    """

    # Category keywords for classification
    CATEGORY_KEYWORDS = {
        'arithmetic': ['sum', 'product', 'add', 'subtract', 'multiply', 'divide', 'calculate', 'compute'],
        'algebra': ['equation', 'solve for', 'variable', 'polynomial', 'quadratic', 'linear', 'factor'],
        'geometry': ['triangle', 'circle', 'angle', 'area', 'perimeter', 'polygon', 'radius', 'diameter'],
        'combinatorics': ['ways', 'arrangements', 'permutation', 'combination', 'choose', 'probability', 'count'],
        'number_theory': ['prime', 'divisible', 'gcd', 'lcm', 'modulo', 'remainder', 'integer', 'divisor'],
        'calculus': ['derivative', 'integral', 'limit', 'continuous', 'differentiate', 'integrate'],
        'proof': ['prove', 'show that', 'demonstrate', 'verify', 'establish', 'must show']
    }

    # Complexity indicators
    COMPLEXITY_KEYWORDS = ['prove', 'show', 'all', 'every', 'maximum', 'minimum',
                          'optimize', 'general', 'arbitrary', 'infinite', 'exists']
    CASE_KEYWORDS = ['if', 'when', 'case', 'either', 'or', 'otherwise', 'suppose']
    STEP_KEYWORDS = ['then', 'next', 'first', 'second', 'finally', 'therefore', 'hence']

    def __init__(self):
        self.difficulty_history = deque(maxlen=100)
        self.category_difficulty_map = {cat: [] for cat in self.CATEGORY_KEYWORDS}

    def extract_features(self, problem_text: str) -> ProblemFeatures:
        """Extract quantifiable features from problem text."""
        features = ProblemFeatures()
        text_lower = problem_text.lower()

        # Basic metrics
        features.char_count = len(problem_text)
        features.word_count = len(problem_text.split())

        # Mathematical complexity
        features.equation_count = len(re.findall(r'[=<>≤≥≠]', problem_text))
        features.variable_count = len(set(re.findall(r'\b[a-z]\b', text_lower)))
        features.number_count = len(re.findall(r'\d+', problem_text))

        # Structural complexity
        features.parentheses_depth = problem_text.count('(') + problem_text.count('[') + problem_text.count('{')
        features.fraction_count = len(re.findall(r'\\frac|\\dfrac|\\tfrac|/', problem_text))

        # Multi-part indicators
        features.part_count = len(re.findall(r'\$[a-z]\$|\$[ivx]+\$|\d+\$', problem_text))
        features.question_count = problem_text.count('?')

        # Keyword-based complexity
        features.complexity_keywords = sum(1 for kw in self.COMPLEXITY_KEYWORDS if kw in text_lower)
        features.case_indicators = sum(1 for kw in self.CASE_KEYWORDS if kw in text_lower)
        features.step_indicators = sum(1 for kw in self.STEP_KEYWORDS if kw in text_lower)


        # Classify category
        features.category = self._classify_category(text_lower)

        return features

    def _classify_category(self, text_lower: str) -> str:
        """Classify problem into mathematical category."""
        scores = {}
        for cat, keywords in self.CATEGORY_KEYWORDS.items():
            scores[cat] = sum(1 for kw in keywords if kw in text_lower)

        best_cat = max(scores, key=scores.get)
        return best_cat if scores[best_cat] > 0 else 'unknown'

    def assess_difficulty(self, problem_text: str, features: Optional[ProblemFeatures] = None) -> Tuple[DifficultyLevel, float, Dict]:
        """
        Assess problem difficulty level.
        Returns: (difficulty_level, confidence_in_assessment, details)
        """
        if features is None:
            features = self.extract_features(problem_text)

        # Calculate complexity score (0-100)
        complexity_score = self._calculate_complexity_score(features)

        # Map to difficulty level
        if complexity_score < 15:
            level = DifficultyLevel.TRIVIAL
        elif complexity_score < 30:
            level = DifficultyLevel.EASY
        elif complexity_score < 50:
            level = DifficultyLevel.MEDIUM
        elif complexity_score < 70:
            level = DifficultyLevel.HARD
        elif complexity_score < 85:
            level = DifficultyLevel.VERY_HARD
        else:
            level = DifficultyLevel.EXTREME

        # Calculate confidence in this assessment
        assessment_confidence = self._calculate_assessment_confidence(features, level)

        details = {
            'complexity_score': complexity_score,
            'features': features.to_dict(),
            'historical_accuracy': self._get_historical_accuracy(features.category)
        }

        return level, assessment_confidence, details

    def _calculate_complexity_score(self, features: ProblemFeatures) -> float:
        """Calculate numerical complexity score from features."""
        score = 0.0

        # Length-based (0-20 points)
        score += min(20, features.word_count / 10)

        # Equation complexity (0-20 points)
        score += min(20, features.equation_count * 3 + features.variable_count * 2)

        # Structural complexity (0-20 points)
        score += min(20, features.parentheses_depth * 2 + features.fraction_count * 3)

        # Multi-part (0-15 points)
        score += min(15, features.part_count * 5)

        # Keyword complexity (0-25 points)
        score += min(25, features.complexity_keywords * 5 +
                    features.case_indicators * 3 +
                    features.step_indicators * 2)

        return min(100, score)

    def _calculate_assessment_confidence(self, features: ProblemFeatures, level: DifficultyLevel) -> float:
        """Calculate confidence in difficulty assessment."""
        # Base confidence
        confidence = 0.5

        # More features = more confident
        feature_count = sum([
            features.equation_count > 0,
            features.complexity_keywords > 0,
            features.part_count > 0,
            features.word_count > 20
        ])
        confidence += feature_count * 0.1

        # Historical data improves confidence
        if features.category in self.category_difficulty_map:
            history = self.category_difficulty_map[features.category]
            if len(history) >= 5:
                confidence += 0.1

        return min(0.95, confidence)

    def _get_historical_accuracy(self, category: str) -> float:
        """Get historical prediction accuracy for category."""
        if category not in self.category_difficulty_map:
            return 0.5
        history = self.category_difficulty_map[category]
        if not history:
            return 0.5
        return np.mean([h.get('accuracy', 0.5) for h in history[-10:]])

    def update_history(self, features: ProblemFeatures, predicted_difficulty: DifficultyLevel,
                      actual_tokens: int, was_correct: bool):
        """Update historical data after problem completion."""
        record = {
            'difficulty': predicted_difficulty.value,
            'actual_tokens': actual_tokens,
            'correct': was_correct,
            'accuracy': 1.0 if was_correct else 0.0
        }
        self.difficulty_history.append(record)

        if features.category in self.category_difficulty_map:
            self.category_difficulty_map[features.category].append(record)

    def get_token_estimate_for_difficulty(self, level: DifficultyLevel) -> int:
        """Get base token estimate for difficulty level."""
        estimates = {
            DifficultyLevel.TRIVIAL: 150,
            DifficultyLevel.EASY: 300,
            DifficultyLevel.MEDIUM: 500,
            DifficultyLevel.HARD: 800,
            DifficultyLevel.VERY_HARD: 1200,
            DifficultyLevel.EXTREME: 2000
        }
        return estimates.get(level, 500)


# ==========================================
# MODULE 2: CONFIDENCE ASSESSMENT
# ==========================================
class DualMetricConfidenceAssessor:
    """
    Implements the Dual Metric Confidence Assessment from the proposal:
    1. Self-reported confidence: Model's own estimate
    2. Peer review analysis: Cross-validation and historical accuracy
    """

    def __init__(self, ema_alpha: float = 0.3):
        self.ema_alpha = ema_alpha

        # Historical tracking for peer review
        self.prediction_history = deque(maxlen=50)
        self.category_performance = {}
        self.global_calibration = {
            'bias': 0.0,           # Positive = over-predicting
            'accuracy': 0.5,       # Overall prediction accuracy
            'confidence': 0.5      # Confidence in our confidence estimates
        }

        # Peer review weights
        self.peer_weights = {
            'historical_accuracy': 0.3,
            'category_performance': 0.3,
            'recent_trend': 0.2,
            'calibration_quality': 0.2
        }

    def assess_confidence(self,
                         problem_text: str,
                         features: ProblemFeatures,
                         manager_response: str,
                         base_prediction: int,
                         learner_prediction: int) -> ConfidenceScore:
        """
        Perform dual metric confidence assessment.

        Args:
            problem_text: The problem to solve
            features: Extracted problem features
            manager_response: Raw response from manager model
            base_prediction: Manager's token prediction
            learner_prediction: Pattern-based prediction

        Returns:
            ConfidenceScore with self-reported and peer-review components
        """
        # 1. Self-Reported Confidence
        self_confidence, self_details = self._assess_self_reported(
            manager_response, base_prediction, learner_prediction
        )

        # 2. Peer Review Confidence
        peer_confidence, peer_details = self._assess_peer_review(
            features, base_prediction
        )

        # 3. Combine with weighted average
        # Weight self-reported more when we have less historical data
        history_weight = min(1.0, len(self.prediction_history) / 20)
        self_weight = 0.7 - (0.2 * history_weight)  # 0.7 -> 0.5 as history grows
        peer_weight = 1.0 - self_weight

        combined = self_weight * self_confidence + peer_weight * peer_confidence

        return ConfidenceScore(
            self_reported=self_confidence,
            peer_review=peer_confidence,
            combined=combined,
            method="dual_metric",
            details={
                'self_details': self_details,
                'peer_details': peer_details,
                'weights': {'self': self_weight, 'peer': peer_weight}
            }
        )

    def _assess_self_reported(self, manager_response: str,
                              base_prediction: int,
                              learner_prediction: int) -> Tuple[float, Dict]:
        """
        Assess self-reported confidence from model's response.
        """
        details = {}
        confidence = 0.5

        # 1. Extract explicit confidence statement
        confidence_match = re.search(r'\[CONFIDENCE\]\s*(low|medium|high)', manager_response, re.IGNORECASE)
        if confidence_match:
            stated = confidence_match.group(1).lower().replace(' ', '')
            confidence_map = {'low': 0.3, 'medium': 0.5, 'high': 0.7, 'veryhigh': 0.85}
            confidence = confidence_map.get(stated, 0.5)
            details['stated_confidence'] = stated

        # 2. Check prediction agreement (manager vs learner)
        if base_prediction > 0 and learner_prediction > 0:
            ratio = min(base_prediction, learner_prediction) / max(base_prediction, learner_prediction)
            agreement_bonus = (ratio - 0.5) * 0.3  # -0.15 to +0.15
            confidence += agreement_bonus
            details['prediction_agreement'] = ratio

        # 3. Check for uncertainty language
        uncertainty_phrases = ['approximately', 'around', 'roughly', 'maybe', 'possibly',
                             'uncertain', 'not sure', 'difficult to estimate']
        uncertainty_count = sum(1 for phrase in uncertainty_phrases
                               if phrase in manager_response.lower())
        if uncertainty_count > 0:
            confidence -= uncertainty_count * 0.05
            details['uncertainty_signals'] = uncertainty_count

        # 4. Check for certainty language
        certainty_phrases = ['definitely', 'certainly', 'clearly', 'obviously',
                            'straightforward', 'simple']
        certainty_count = sum(1 for phrase in certainty_phrases
                             if phrase in manager_response.lower())
        if certainty_count > 0:
            confidence += certainty_count * 0.03
            details['certainty_signals'] = certainty_count

        confidence = max(0.1, min(0.95, confidence))
        return confidence, details

    def _assess_peer_review(self, features: ProblemFeatures,
                           base_prediction: int) -> Tuple[float, Dict]:
        """
        Assess confidence based on historical performance (peer review).
        """
        details = {}
        scores = []

        # 1. Historical accuracy for this category
        category = features.category
        if category in self.category_performance:
            cat_perf = self.category_performance[category]
            if cat_perf['count'] >= 3:
                hist_accuracy = cat_perf['accuracy']
                scores.append(('historical_accuracy', hist_accuracy, self.peer_weights['historical_accuracy']))
                details['category_accuracy'] = hist_accuracy
                details['category_count'] = cat_perf['count']

        # 2. Recent prediction trend
        if len(self.prediction_history) >= 5:
            recent = list(self.prediction_history)[-10:]
            recent_accuracy = np.mean([1.0 - min(abs(p['error_pct']), 1.0) for p in recent])
            scores.append(('recent_trend', recent_accuracy, self.peer_weights['recent_trend']))
            details['recent_accuracy'] = recent_accuracy

        # 3. Calibration quality (how well-calibrated are our predictions?)
        calibration_score = 1.0 - min(abs(self.global_calibration['bias']), 0.5) * 2
        scores.append(('calibration', calibration_score, self.peer_weights['calibration_quality']))
        details['calibration_bias'] = self.global_calibration['bias']

        # 4. Prediction magnitude confidence
        # We're more confident about medium-range predictions
        if 300 <= base_prediction <= 1500:
            magnitude_conf = 0.7
        elif 150 <= base_prediction <= 2500:
            magnitude_conf = 0.5
        else:
            magnitude_conf = 0.3
        scores.append(('magnitude', magnitude_conf, 0.2))
        details['prediction_magnitude'] = base_prediction

        # Calculate weighted average
        if scores:
            total_weight = sum(w for _, _, w in scores)
            peer_confidence = sum(s * w for _, s, w in scores) / total_weight
        else:
            peer_confidence = 0.5

        return max(0.1, min(0.95, peer_confidence)), details

    def update_from_result(self, features: ProblemFeatures, predicted: int,
                          actual: int, was_correct: bool):
        """Update peer review data after problem completion."""
        error_pct = (predicted - actual) / max(actual, 1)
        accuracy = 1.0 - min(abs(error_pct), 1.0)

        # Update prediction history
        self.prediction_history.append({
            'predicted': predicted,
            'actual': actual,
            'error_pct': error_pct,
            'accuracy': accuracy,
            'correct': was_correct,
            'category': features.category
        })

        # Update category performance
        category = features.category
        if category not in self.category_performance:
            self.category_performance[category] = {'accuracy': 0.5, 'count': 0, 'sum': 0}

        cat_perf = self.category_performance[category]
        cat_perf['count'] += 1
        cat_perf['sum'] += accuracy
        cat_perf['accuracy'] = cat_perf['sum'] / cat_perf['count']

        # Update global calibration with EMA
        self.global_calibration['bias'] = (
            self.ema_alpha * error_pct +
            (1 - self.ema_alpha) * self.global_calibration['bias']
        )
        self.global_calibration['accuracy'] = (
            self.ema_alpha * accuracy +
            (1 - self.ema_alpha) * self.global_calibration['accuracy']
        )


# ==========================================
# MODULE 3: PROBABILISTIC TOKEN ALLOCATION
# ==========================================
class ProbabilisticTokenAllocator:
    """
    Implements: p% confidence → q% probability of token allocation

    This module translates confidence scores into actual token budgets
    using probabilistic allocation strategies.
    """

    def __init__(self, config: Config = None):
        self.config = config or Config()

        # Allocation history for learning
        self.allocation_history = deque(maxlen=100)

        # Learned adjustments per category
        self.category_adjustments = {}

        # Safety factor bounds
        self.min_safety = 1.1
        self.max_safety = 2.0

    def allocate_tokens(self,
                       base_prediction: int,
                       confidence: ConfidenceScore,
                       difficulty: DifficultyLevel,
                       features: ProblemFeatures) -> TokenAllocation:
        """
        Allocate tokens based on confidence and difficulty.

        Implements: p% confidence → q% probability of additional tokens
        """

                # 1. Apply confidence-based adjustment to base prediction
        confidence_adjusted = self._apply_confidence_adjustment(
            base_prediction, confidence.combined
        )

        # 2. Calculate safety factor based on confidence
        safety_factor = self._calculate_safety_factor(
            confidence.combined, difficulty, features.category
        )

        # 3. Apply category-specific learned adjustments
        category_multiplier = self._get_category_multiplier(features.category)

        # 4. Calculate final budget
        final_budget = int(confidence_adjusted * safety_factor * category_multiplier)

        # 5. Clamp to valid range
        final_budget = max(self.config.MIN_TOKENS, min(final_budget, self.config.MAX_TOKENS))

        # 6. Generate allocation reason
        allocation_reason = self._generate_allocation_reason(
            base_prediction, confidence, difficulty, safety_factor, final_budget
        )

        return TokenAllocation(
            base_prediction=base_prediction,
            confidence_adjusted=confidence_adjusted,
            safety_factor=safety_factor,
            final_budget=final_budget,
            difficulty_level=difficulty,
            allocation_reason=allocation_reason
        )

    def _apply_confidence_adjustment(self, base_prediction: int, confidence: float) -> int:
        """
        Apply confidence-based adjustment to base prediction.

        p% confidence → q% of base prediction
        - High confidence (>80%): Use 100% of prediction
        - Medium confidence (50-80%): Use 100-120% of prediction
        - Low confidence (<50%): Use 120-150% of prediction
        """
        if confidence >= 0.8:
            # High confidence: trust the prediction
            multiplier = 1.0
        elif confidence >= 0.5:
            # Medium confidence: slight buffer
            # Linear interpolation: 0.8 -> 1.0, 0.5 -> 1.2
            multiplier = 1.0 + (0.8 - confidence) * (0.2 / 0.3)
        else:
            # Low confidence: significant buffer
            # Linear interpolation: 0.5 -> 1.2, 0.0 -> 1.5
            multiplier = 1.2 + (0.5 - confidence) * (0.3 / 0.5)

        return int(base_prediction * multiplier)

    def _calculate_safety_factor(self, confidence: float,
                                 difficulty: DifficultyLevel,
                                 category: str) -> float:
        """
        Calculate safety factor based on multiple signals.

        Lower confidence = higher safety factor
        Higher difficulty = higher safety factor
        """
        # Base safety from confidence
        # confidence 0.9 -> safety 1.1
        # confidence 0.3 -> safety 1.8
        base_safety = self.max_safety - (confidence * (self.max_safety - self.min_safety))

        # Difficulty adjustment
        difficulty_multipliers = {
            DifficultyLevel.TRIVIAL: 0.9,
            DifficultyLevel.EASY: 0.95,
            DifficultyLevel.MEDIUM: 1.0,
            DifficultyLevel.HARD: 1.05,
            DifficultyLevel.VERY_HARD: 1.1,
            DifficultyLevel.EXTREME: 1.15
        }
        difficulty_mult = difficulty_multipliers.get(difficulty, 1.0)

        # Category adjustment (learned from history)
        category_safety = self.category_adjustments.get(category, {}).get('safety_adj', 1.0)

        final_safety = base_safety * difficulty_mult * category_safety
        return max(self.min_safety, min(self.max_safety, final_safety))

    def _get_category_multiplier(self, category: str) -> float:
        """Get learned category-specific multiplier."""
        if category not in self.category_adjustments:
            return 1.0

        adj = self.category_adjustments[category]
        if adj.get('count', 0) < 3:
            return 1.0

        # Use historical ratio of actual/predicted
        return adj.get('avg_ratio', 1.0)

    def _generate_allocation_reason(self, base: int, confidence: ConfidenceScore,
                                   difficulty: DifficultyLevel, safety: float,
                                   final: int) -> str:
        """Generate human-readable allocation reason."""
        conf_level = "high" if confidence.combined >= 0.7 else "medium" if confidence.combined >= 0.4 else "low"

        return (f"Base: {base} tokens | "
                f"Confidence: {confidence.combined:.0%} ({conf_level}) | "
                f"Difficulty: {difficulty.name} | "
                f"Safety: {safety:.2f}x | "
                f"Final: {final} tokens")

    def update_from_result(self, allocation: TokenAllocation, actual_tokens: int,
                          was_correct: bool, hit_limit: bool, category: str):
        """Update allocation model from execution result."""
        # Record allocation result
        result = {
            'predicted': allocation.base_prediction,
            'allocated': allocation.final_budget,
            'actual': actual_tokens,
            'correct': was_correct,
            'hit_limit': hit_limit,
            'ratio': actual_tokens / max(allocation.base_prediction, 1),
            'efficiency': actual_tokens / max(allocation.final_budget, 1),
            'category': category
        }
        self.allocation_history.append(result)

        # Update category adjustments
        if category not in self.category_adjustments:
            self.category_adjustments[category] = {
                'count': 0,
                'sum_ratio': 0,
                'avg_ratio': 1.0,
                'safety_adj': 1.0
            }

        adj = self.category_adjustments[category]
        adj['count'] += 1
        adj['sum_ratio'] += result['ratio']
        adj['avg_ratio'] = adj['sum_ratio'] / adj['count']

        # Adjust safety if we're consistently hitting limits or wasting tokens
        if hit_limit and not was_correct:
            adj['safety_adj'] = min(1.5, adj['safety_adj'] * 1.1)
        elif result['efficiency'] < 0.5:  # Used less than 50% of budget
            adj['safety_adj'] = max(0.8, adj['safety_adj'] * 0.95)

    def get_probabilistic_budget(self, base_prediction: int,
                                 confidence: float) -> Tuple[int, float]:
        """
        Get budget with probability of needing more tokens.

        Returns: (budget, probability_of_needing_more)
        """
        # Calculate probability of needing more tokens based on confidence
        # Low confidence = high probability of needing more
        prob_need_more = 1.0 - confidence

        # Adjust budget based on this probability
        # If 70% chance of needing more, allocate 70% more buffer
        buffer_multiplier = 1.0 + (prob_need_more * 0.8)

        budget = int(base_prediction * buffer_multiplier)
        budget = max(self.config.MIN_TOKENS, min(budget, self.config.MAX_TOKENS))

        return budget, prob_need_more


class SequentialHandler:
    """
    Manages sequential problem handling with:
    - Dependency tracking
    - Cascading failure prevention
    - Adaptive retry strategies
    """

    def __init__(self, config: Config = None):
        self.config = config or Config()
        self.state = SequenceState()

        # Cascade risk thresholds
        self.cascade_warning_threshold = 0.3
        self.cascade_critical_threshold = 0.6

        # Adaptive parameters
        self.base_retry_budget_multiplier = 2.0
        self.consecutive_failure_penalty = 0.1

    def update_state(self, result: ExecutionResult):
        """Update sequence state after problem execution."""
        self.state.problem_idx += 1

        if result.success:
            self.state.cumulative_success += 1
            self.state.consecutive_failures = 0

            # Track token efficiency
            waste = max(0, result.budget_used - result.actual_tokens)
            self.state.tokens_wasted += waste

            saved = max(0, self.config.MAX_TOKENS - result.budget_used)
            self.state.tokens_saved += saved
        else:
            self.state.cumulative_failures += 1
            self.state.consecutive_failures += 1

        # Update cascade risk
        self._update_cascade_risk()

    def _update_cascade_risk(self):
        """Calculate risk of cascading failures."""
        if self.state.problem_idx == 0:
            self.state.cascade_risk = 0.0
            return

        # Factors contributing to cascade risk:
        # 1. Consecutive failures
        consecutive_factor = min(1.0, self.state.consecutive_failures * 0.2)

        # 2. Overall failure rate
        failure_rate = self.state.cumulative_failures / self.state.problem_idx

        # 3. Recent trend (weighted more heavily)
        recent_weight = 0.6
        overall_weight = 0.4

        self.state.cascade_risk = (
            recent_weight * consecutive_factor +
            overall_weight * failure_rate
        )

    def get_retry_strategy(self, attempt: int, hit_limit: bool,
                          current_budget: int) -> Tuple[bool, int, str]:
        """
        Determine retry strategy based on failure type and history.

        Returns: (should_retry, new_budget, reason)
        """
        if attempt >= self.config.MAX_RETRIES:
            return False, current_budget, "Max retries exceeded"

        # Adjust retry aggressiveness based on cascade risk
        cascade_adjustment = 1.0 + (self.state.cascade_risk * 0.5)

        if hit_limit:
            # Timeout: need more tokens
            multiplier = self.config.RETRY_MULTIPLIER_TIMEOUT * cascade_adjustment
            new_budget = min(int(current_budget * multiplier), self.config.MAX_TOKENS)
            reason = f"Timeout - increasing budget by {multiplier:.1f}x"
        else:
            # Wrong answer: might need more reasoning space
            multiplier = self.config.RETRY_MULTIPLIER_WRONG * cascade_adjustment
            new_budget = min(int(current_budget * multiplier), self.config.MAX_TOKENS)
            reason = f"Wrong answer - increasing budget by {multiplier:.1f}x"

        # Check if new budget exceeds maximum
        if new_budget >= self.config.MAX_TOKENS and current_budget >= self.config.MAX_TOKENS * 0.9:
            return False, current_budget, "Budget already at maximum"

        return True, new_budget, reason

    def should_apply_conservative_strategy(self) -> bool:
        """Check if we should use conservative token allocation."""
        return self.state.cascade_risk >= self.cascade_warning_threshold

    def get_cascade_adjustment(self) -> float:
        """Get multiplier adjustment based on cascade risk."""
        if self.state.cascade_risk < self.cascade_warning_threshold:
            return 1.0
        elif self.state.cascade_risk < self.cascade_critical_threshold:
            return 1.2  # 20% more conservative
        else:
            return 1.5  # 50% more conservative

    def get_state_summary(self) -> Dict:
        """Get summary of current sequence state."""
        total = self.state.problem_idx
        if total == 0:
            success_rate = 0.0
        else:
            success_rate = self.state.cumulative_success / total

        return {
            'problems_processed': total,
            'success_rate': f"{success_rate:.1%}",
            'consecutive_failures': self.state.consecutive_failures,
            'cascade_risk': f"{self.state.cascade_risk:.1%}",
            'tokens_saved': self.state.tokens_saved,
            'tokens_wasted': self.state.tokens_wasted,
            'efficiency': f"{self.state.tokens_saved / max(1, self.state.tokens_saved + self.state.tokens_wasted):.1%}"
        }


# ==========================================
# MODULE 4: PENALTY & REWARD MODEL
# ==========================================
class PenaltyRewardModel:
    """
    Comprehensive penalty and reward system for token allocation.
    Encourages efficient predictions without sacrificing correctness.
    """

    class PenaltyType(Enum):
        CRITICAL_UNDERPREDICTION = "CRITICAL_UNDERPREDICTION"
        SEVERE_OVERPREDICTION = "SEVERE_OVERPREDICTION"
        MODERATE_OVERPREDICTION = "MODERATE_OVERPREDICTION"
        BORDERLINE = "BORDERLINE"
        OPTIMAL = "OPTIMAL"
        EXCELLENT = "EXCELLENT"

    def __init__(self, config: Config = None):
        self.config = config or Config()
        self.penalty_history = deque(maxlen=100)

        # Cumulative metrics
        self.cumulative_waste = 0
        self.cumulative_shortage = 0
        self.cumulative_penalty = 0.0
        self.cumulative_reward = 0.0

    def calculate_penalty(self, predicted: int, actual: int,
                         budget: int, was_correct: bool,
                         hit_limit: bool) -> Tuple[float, str, Dict]:
        """
        Calculate penalty/reward for a prediction.

        Returns: (penalty_score, penalty_type, details)
        """
        waste = max(0, budget - actual)
        shortage = max(0, actual - budget)
        prediction_error = abs(predicted - actual) / max(actual, 1)

        penalty = 0.0
        penalty_type = self.PenaltyType.OPTIMAL
        details = {}

        if hit_limit and not was_correct:
            # Critical: ran out of tokens and failed
            penalty = self.config.UNDERPREDICTION_PENALTY + (shortage / max(predicted, 1))
            penalty_type = self.PenaltyType.CRITICAL_UNDERPREDICTION
            self.cumulative_shortage += shortage
            details = {
                'shortage': shortage,
                'severity': 'critical',
                'recommendation': 'Significantly increase predictions for similar problems'
            }

        elif hit_limit and was_correct:
            # Borderline: barely made it
            penalty = 0.5
            penalty_type = self.PenaltyType.BORDERLINE
            details = {
                'note': 'Succeeded despite hitting limit',
                'recommendation': 'Slight increase recommended'
            }

        elif waste > budget * 0.5:
            # Severe over-prediction
            penalty = self.config.OVERPREDICTION_PENALTY + (waste / max(actual, 1)) * 0.5
            penalty_type = self.PenaltyType.SEVERE_OVERPREDICTION
            self.cumulative_waste += waste
            details = {
                'waste': waste,
                'waste_pct': f"{waste/budget:.1%}",
                'recommendation': 'Significantly reduce predictions'
            }

        elif waste > budget * 0.3:
            # Moderate over-prediction
            penalty = 0.3 + (waste / max(actual, 1)) * 0.2
            penalty_type = self.PenaltyType.MODERATE_OVERPREDICTION
            self.cumulative_waste += waste
            details = {
                'waste': waste,
                'waste_pct': f"{waste/budget:.1%}",
                'recommendation': 'Slightly reduce predictions'
            }

        elif prediction_error <= 0.2:
            # Excellent prediction: within 20%
            penalty = self.config.EXCELLENT_REWARD  # Negative = reward
            penalty_type = self.PenaltyType.EXCELLENT
            self.cumulative_reward += abs(penalty)
            details = {
                'accuracy': f"{1 - prediction_error:.1%}",
                'note': 'Excellent prediction accuracy'
            }

        else:
            # Optimal: acceptable range
            penalty_type = self.PenaltyType.OPTIMAL
            details = {'note': 'Acceptable prediction'}

        # Update cumulative penalty
        self.cumulative_penalty += max(0, penalty)

        # Record in history
        self.penalty_history.append({
            'penalty': penalty,
            'type': penalty_type.value,
            'predicted': predicted,
            'actual': actual,
            'budget': budget,
            'correct': was_correct
        })

        return penalty, penalty_type.value, details

    def get_adjustment_recommendation(self) -> Tuple[float, str]:
        """
        Analyze recent penalties and recommend prediction adjustment.

        Returns: (adjustment_factor, reason)
        """
        if len(self.penalty_history) < 5:
            return 0.0, "INSUFFICIENT_DATA"

        recent = list(self.penalty_history)[-15:]

        # Count penalty types
        over_count = sum(1 for p in recent if 'OVER' in p['type'])
        under_count = sum(1 for p in recent if 'UNDER' in p['type'])
        excellent_count = sum(1 for p in recent if p['type'] == 'EXCELLENT')

        # Calculate average error direction
        avg_error = np.mean([(p['predicted'] - p['actual']) / max(p['actual'], 1)
                           for p in recent])

        if under_count >= 3:
            # Underpredicting too often - increase
            adjustment = 0.15 + (under_count * 0.03)
            return min(0.4, adjustment), "INCREASE_PREDICTIONS"

        elif over_count >= 8:
            # Severely overpredicting - decrease significantly
            adjustment = -0.15 - (over_count * 0.02)
            return max(-0.3, adjustment), "REDUCE_PREDICTIONS"

        elif over_count >= 5:
            # Moderately overpredicting - decrease slightly
            return -0.08, "SLIGHT_REDUCTION"

        elif excellent_count >= 5:
            # Doing well - maintain
            return 0.0, "MAINTAIN_EXCELLENT"

        elif avg_error > 0.2:
            # Trending toward overprediction
            return -0.05, "SLIGHT_REDUCTION"

        elif avg_error < -0.2:
            # Trending toward underprediction
            return 0.08, "SLIGHT_INCREASE"

        return 0.0, "MAINTAIN"

    def get_efficiency_score(self) -> float:
        """Calculate overall efficiency score (0-1)."""
        if not self.penalty_history:
            return 0.5

        recent = list(self.penalty_history)[-20:]

        # Calculate efficiency based on multiple factors
        # 1. Penalty-based score
        avg_penalty = np.mean([p['penalty'] for p in recent])
        penalty_score = max(0, 1 - (avg_penalty / 2))

        # 2. Accuracy-based score
        correct_rate = np.mean([1 if p['correct'] else 0 for p in recent])

        # 3. Waste-based score
        avg_efficiency = np.mean([p['actual'] / max(p['budget'], 1) for p in recent])

        # Weighted combination
        efficiency = (
            0.4 * penalty_score +
            0.4 * correct_rate +
            0.2 * avg_efficiency
        )

        return max(0, min(1, efficiency))

    def get_summary(self) -> Dict:
        """Get penalty model summary."""
        if not self.penalty_history:
            return {'status': 'No data yet'}

        recent = list(self.penalty_history)[-20:]

        return {
            'total_predictions': len(self.penalty_history),
            'cumulative_waste': self.cumulative_waste,
            'cumulative_shortage': self.cumulative_shortage,
            'net_penalty': self.cumulative_penalty - self.cumulative_reward,
            'efficiency_score': f"{self.get_efficiency_score():.1%}",
            'recent_excellent': sum(1 for p in recent if p['type'] == 'EXCELLENT'),
            'recent_critical': sum(1 for p in recent if 'CRITICAL' in p['type']),
            'adjustment_recommendation': self.get_adjustment_recommendation()
        }


class OutputPatternLearner:
    """
    Learns patterns from actual outputs to improve future predictions.
    Addresses: "We need clever methods to translate abstract features into actual inputs"
    """

    def __init__(self):
        self.pattern_database = []
        self.feature_token_correlations = {}
        self.learned_multipliers = {}
        self.category_patterns = {}

    def analyze_output(self, problem_text: str, solution_text: str,
                      features: ProblemFeatures, actual_tokens: int) -> Dict:
        """
        Analyze completed solution to learn patterns.
        """
        # Analyze solution structure
        solution_analysis = self._analyze_solution_structure(solution_text)

        # Calculate complexity metrics
        complexity_score = self._calculate_complexity_score(features)
        tokens_per_complexity = actual_tokens / max(complexity_score, 1)
        tokens_per_word = actual_tokens / max(features.word_count, 1)

        # Create pattern record
        pattern = {
            'features': features.to_dict(),
            'solution_analysis': solution_analysis,
            'actual_tokens': actual_tokens,
            'complexity_score': complexity_score,
            'tokens_per_complexity': tokens_per_complexity,
            'tokens_per_word': tokens_per_word,
            'category': features.category
        }

        self.pattern_database.append(pattern)

        # Update learned correlations
        self._update_correlations(features, actual_tokens)

        # Update category patterns
        self._update_category_patterns(features.category, pattern)

        return pattern

    def _analyze_solution_structure(self, solution_text: str) -> Dict:
        """Analyze the structure of a solution."""
        text_lower = solution_text.lower()

        return {
            'total_length': len(solution_text),
            'step_count': len(re.findall(r'step\s*\d|\\item|\n\d+\.', text_lower)),
            'equation_count': len(re.findall(r'=', solution_text)),
            'case_count': len(re.findall(r'case\s*\d|if\s+.*:', text_lower)),
            'substitution_count': len(re.findall(r'substitut|plug|replace', text_lower)),
            'verification_count': len(re.findall(r'verify|check|confirm', text_lower)),
            'has_boxed_answer': '\\boxed' in solution_text
        }

    def _calculate_complexity_score(self, features: ProblemFeatures) -> float:
        """Calculate numerical complexity score."""
        return (
            features.equation_count * 2 +
            features.variable_count * 1.5 +
            features.complexity_keywords * 4 +
            features.case_indicators * 3 +
            features.part_count * 5 +
            features.word_count * 0.1
        )

    def _update_correlations(self, features: ProblemFeatures, actual_tokens: int):
        """Update feature-token correlations."""
        feature_dict = features.to_dict()

        for feature_name, value in feature_dict.items():
            if isinstance(value, (int, float)) and value > 0:
                if feature_name not in self.feature_token_correlations:
                    self.feature_token_correlations[feature_name] = {
                        'sum_tokens': 0,
                        'sum_feature': 0,
                        'count': 0,
                        'pairs': []
                    }

                corr = self.feature_token_correlations[feature_name]
                corr['sum_tokens'] += actual_tokens
                corr['sum_feature'] += value
                corr['count'] += 1
                corr['pairs'].append((value, actual_tokens))

                # Keep only recent pairs
                if len(corr['pairs']) > 50:
                    corr['pairs'] = corr['pairs'][-50:]

    def _update_category_patterns(self, category: str, pattern: Dict):
        """Update category-specific patterns."""
        if category not in self.category_patterns:
            self.category_patterns[category] = {
                'patterns': [],
                'avg_tokens': 0,
                'avg_complexity': 0
            }

        cat_data = self.category_patterns[category]
        cat_data['patterns'].append(pattern)

        # Keep only recent patterns
        if len(cat_data['patterns']) > 30:
            cat_data['patterns'] = cat_data['patterns'][-30:]

        # Update averages
        patterns = cat_data['patterns']
        cat_data['avg_tokens'] = np.mean([p['actual_tokens'] for p in patterns])
        cat_data['avg_complexity'] = np.mean([p['complexity_score'] for p in patterns])

    def predict_tokens(self, problem_text: str,
                      features: ProblemFeatures) -> Tuple[int, float, str]:
        """
        Predict token count based on learned patterns.

        Returns: (prediction, confidence, method)
        """
        if len(self.pattern_database) < 5:
            return self._heuristic_prediction(features), 0.3, "HEURISTIC"

        # Try pattern matching first
        prediction, confidence = self._pattern_based_prediction(features)
        if confidence >= 0.5:
            return prediction, confidence, "PATTERN_MATCHED"

        # Try category-based prediction
        if features.category in self.category_patterns:
            cat_prediction, cat_confidence = self._category_based_prediction(features)
            if cat_confidence >= 0.4:
                # Blend with pattern prediction
                blended = int(0.6 * cat_prediction + 0.4 * prediction)
                return blended, max(confidence, cat_confidence), "CATEGORY_BLENDED"

        # Fall back to correlation-based prediction
        corr_prediction = self._correlation_based_prediction(features)
        return corr_prediction, 0.4, "CORRELATION_BASED"

    def _heuristic_prediction(self, features: ProblemFeatures) -> int:
        """Simple heuristic prediction when no history available."""
        base = features.word_count * 5

        complexity_bonus = (
            features.equation_count * 40 +
            features.complexity_keywords * 100 +
            features.case_indicators * 80 +
            features.part_count * 120 +
            features.variable_count * 20
        )

        return max(150, min(3000, int(base + complexity_bonus)))

    def _pattern_based_prediction(self, features: ProblemFeatures) -> Tuple[int, float]:
        """Predict based on similar patterns."""
        feature_dict = features.to_dict()

        # Find similar patterns
        similarities = []
        for pattern in self.pattern_database:
            sim_score = self._calculate_similarity(feature_dict, pattern['features'])
            similarities.append((sim_score, pattern))

        # Sort by similarity
        similarities.sort(key=lambda x: x[0], reverse=True)
        top_k = similarities[:min(7, len(similarities))]

        if not top_k or top_k[0][0] < 0.3:
            return self._heuristic_prediction(features), 0.2

        # Weighted average
        total_weight = sum(s[0] for s in top_k)
        if total_weight == 0:
            return self._heuristic_prediction(features), 0.2

        weighted_prediction = sum(s[0] * s[1]['actual_tokens'] for s in top_k) / total_weight
        confidence = min(0.85, top_k[0][0] * 1.1)

        return int(weighted_prediction), confidence

    def _category_based_prediction(self, features: ProblemFeatures) -> Tuple[int, float]:
        """Predict based on category statistics."""
        cat_data = self.category_patterns.get(features.category)
        if not cat_data or len(cat_data['patterns']) < 3:
            return self._heuristic_prediction(features), 0.2

        # Adjust average based on complexity difference
        avg_tokens = cat_data['avg_tokens']
        avg_complexity = cat_data['avg_complexity']
        current_complexity = self._calculate_complexity_score(features)

        if avg_complexity > 0:
            complexity_ratio = current_complexity / avg_complexity
            adjusted_prediction = int(avg_tokens * complexity_ratio)
        else:
            adjusted_prediction = int(avg_tokens)

        confidence = min(0.7, 0.3 + len(cat_data['patterns']) * 0.02)

        return adjusted_prediction, confidence

    def _correlation_based_prediction(self, features: ProblemFeatures) -> int:
        """Predict based on feature-token correlations."""
        feature_dict = features.to_dict()
        predictions = []
        weights = []

        for feature_name, value in feature_dict.items():
            if isinstance(value, (int, float)) and value > 0:
                if feature_name in self.feature_token_correlations:
                    corr = self.feature_token_correlations[feature_name]
                    if corr['count'] >= 3:
                        avg_tokens_per_feature = corr['sum_tokens'] / corr['sum_feature']
                        pred = value * avg_tokens_per_feature
                        predictions.append(pred)
                        weights.append(corr['count'])

        if not predictions:
            return self._heuristic_prediction(features)

        # Weighted average
        total_weight = sum(weights)
        weighted_pred = sum(p * w for p, w in zip(predictions, weights)) / total_weight

        return max(150, min(3000, int(weighted_pred)))

    def _calculate_similarity(self, features1: Dict, features2: Dict) -> float:
        """Calculate similarity between two feature sets."""
        common_keys = set(features1.keys()) & set(features2.keys())
        numeric_keys = [k for k in common_keys
                       if isinstance(features1[k], (int, float)) and
                          isinstance(features2[k], (int, float))]

        if not numeric_keys:
            return 0.0

        similarities = []
        for key in numeric_keys:
            v1, v2 = features1[key], features2[key]
            if v1 == 0 and v2 == 0:
                similarities.append(1.0)
            elif v1 == 0 or v2 == 0:
                similarities.append(0.0)
            else:
                similarities.append(min(v1, v2) / max(v1, v2))

        # Also check category match
        if features1.get('category') == features2.get('category'):
            similarities.append(1.0)
        else:
            similarities.append(0.3)

        return np.mean(similarities)

    def get_insight(self, features: ProblemFeatures) -> str:
        """Generate human-readable insight about expected complexity."""
        insights = []

        if features.complexity_keywords >= 2:
            insights.append("Contains proof/optimization keywords (high complexity)")
        if features.part_count >= 2:
            insights.append(f"Multi-part problem ({features.part_count} parts)")
        if features.case_indicators >= 2:
            insights.append("Likely requires case analysis")
        if features.equation_count >= 4:
            insights.append("Multiple equations to solve")
        if features.variable_count >= 5:
            insights.append("Many variables involved")
        if features.fraction_count >= 2:
            insights.append("Contains fractions (algebraic manipulation)")

        if not insights:
            if features.word_count < 25:
                insights.append("Short problem, likely straightforward")
            elif features.word_count > 100:
                insights.append("Long problem, may require careful reading")
            else:
                insights.append("Standard complexity")

        return "; ".join(insights)

    def get_learning_stats(self) -> Dict:
        """Get statistics about learned patterns."""
        return {
            'total_patterns': len(self.pattern_database),
            'categories_learned': list(self.category_patterns.keys()),
            'feature_correlations': len(self.feature_token_correlations),
            'avg_tokens_overall': np.mean([p['actual_tokens'] for p in self.pattern_database]) if self.pattern_database else 0
        }


# ==========================================
# MODULE 7: REFLEXION MEMORY SYSTEM
# ==========================================
class ReflexionMemory:
    """
    Memory system for storing successful trajectories and failure reflections.
    Enables learning from past experiences.
    """

    def __init__(self, max_successes: int = 7, max_reflections: int = 7):
        self.max_successes = max_successes
        self.max_reflections = max_reflections

        self.success_buffer = []
        self.reflection_buffer = []
        self.prediction_feedback = []

    def _calculate_similarity(self, new_text: str, buffer: List[Dict]) -> float:
        """Calculate max similarity with existing entries."""
        if not new_text or not buffer:
            return 0.0

        new_words = set(new_text.lower().split())
        max_sim = 0.0

        for item in buffer:
            item_text = item.get('text', '')
            item_words = set(item_text.lower().split())

            intersection = len(new_words & item_words)
            union = len(new_words | item_words)

            if union > 0:
                sim = intersection / union
                max_sim = max(max_sim, sim)

        return max_sim

    def add_success(self, problem: str, tokens: int, analysis: str,
                   prediction_accuracy: float, category: str) -> bool:
        """Add successful trajectory to memory."""
        # Check for duplicates
        if self._calculate_similarity(analysis, self.success_buffer) > 0.7:
            return False

        entry = {
            'text': analysis,
            'full_entry': (
                f"[SUCCESS EXEMPLAR - {category.upper()}]\n"
                f"Problem: \"{problem[:120]}...\"\n"
                f"Analysis: {analysis}\n"
                f"Actual Tokens: {tokens}\n"
                f"Prediction Accuracy: {prediction_accuracy:.1%}"
            ),
            'tokens': tokens,
            'category': category,
            'accuracy': prediction_accuracy
        }

        self.success_buffer.append(entry)

        # Remove oldest if over limit
        if len(self.success_buffer) > self.max_successes:
            self.success_buffer.pop(0)

        return True

    def add_reflection(self, problem: str, reflection: str,
                      penalty_info: str, category: str) -> bool:
        """Add failure reflection to memory."""
        # Check for duplicates
        if self._calculate_similarity(reflection, self.reflection_buffer) > 0.7:
            return False

        entry = {
            'text': reflection,
            'full_entry': (
                f"[FAILURE REFLECTION - {category.upper()}]\n"
                f"Problem: \"{problem[:120]}...\"\n"
                f"Insight: {reflection}\n"
                f"Penalty: {penalty_info}"
            ),
            'category': category
        }

        self.reflection_buffer.append(entry)

        if len(self.reflection_buffer) > self.max_reflections:
            self.reflection_buffer.pop(0)

        return True

    def add_prediction_feedback(self, predicted: int, actual: int,
                               category: str, outcome: str):
        """Store prediction feedback for calibration."""
        self.prediction_feedback.append({
            'predicted': predicted,
            'actual': actual,
            'category': category,
            'outcome': outcome,
            'ratio': actual / max(predicted, 1),
            'error_pct': (predicted - actual) / max(actual, 1)
        })

        # Keep recent feedback
        if len(self.prediction_feedback) > 50:
            self.prediction_feedback.pop(0)

    def get_calibration_context(self) -> str:
        """Generate context about prediction calibration."""
        if len(self.prediction_feedback) < 5:
            return ""

        recent = self.prediction_feedback[-15:]
        avg_ratio = np.mean([f['ratio'] for f in recent])
        avg_error = np.mean([f['error_pct'] for f in recent])

        over_predictions = sum(1 for f in recent if f['error_pct'] > 0.3)
        under_predictions = sum(1 for f in recent if f['error_pct'] < -0.3)

        lines = ["\n--- PREDICTION CALIBRATION STATUS ---"]

        if avg_error > 0.2:
            lines.append(f"⚠️ OVER-PREDICTING by ~{avg_error:.0%}. Reduce estimates.")
        elif avg_error < -0.2:
            lines.append(f"⚠️ UNDER-PREDICTING by ~{abs(avg_error):.0%}. Increase estimates.")
        else:
            lines.append("✓ Predictions are well-calibrated.")

        if over_predictions >= 5:
            lines.append(f"Note: {over_predictions}/{len(recent)} predictions were too high.")
        if under_predictions >= 3:
            lines.append(f"⚠️ {under_predictions}/{len(recent)} predictions were too low (risky).")

        return "\n".join(lines)

    def get_context(self) -> str:
        """Get full memory context for prompts."""
        parts = []

        # Add calibration context
        calibration = self.get_calibration_context()
        if calibration:
            parts.append(calibration)

        # Add success exemplars
        if self.success_buffer:
            parts.append("\n--- SUCCESSFUL TRAJECTORIES ---")
            for item in self.success_buffer[-5:]:
                parts.append(item['full_entry'])

        # Add failure reflections
        if self.reflection_buffer:
            parts.append("\n--- FAILURE REFLECTIONS (WARNINGS) ---")
            for item in self.reflection_buffer[-5:]:
                parts.append(item['full_entry'])

        # Default examples if no history
        if not parts:
            return """
--- REFERENCE EXAMPLES ---
[TRIVIAL] "Calculate 15 × 4" → ~100 tokens
[EASY] "Solve x² - 5x + 6 = 0" → ~250 tokens
[MEDIUM] "Find all integer solutions..." → ~500 tokens
[HARD] "Prove that √2 is irrational" → ~800 tokens
[VERY HARD] "Prove for all n..." → ~1200 tokens
"""

        return "\n".join(parts)

    def get_stats(self) -> Dict:
        """Get memory statistics."""
        return {
            'success_count': len(self.success_buffer),
            'reflection_count': len(self.reflection_buffer),
            'feedback_count': len(self.prediction_feedback),
            'categories_seen': list(set(
                [s['category'] for s in self.success_buffer] +
                [r['category'] for r in self.reflection_buffer]
            ))
        }

class TokenParser:
    """
    Multi-strategy token extraction from model responses.
    """

    @staticmethod
    def extract(text: str, fallback_prediction: Optional[int] = None) -> Tuple[int, float, str]:
        """
        Extract token prediction from text using multiple strategies.

        Returns: (tokens, confidence, method)
        """
        if not text:
            return fallback_prediction or 500, 0.2, "EMPTY_INPUT"

        clean_text = text.replace(',', '').lower()

        # Strategy 1: Explicit tag matching [TOKENS] 800
        match = re.search(r'\$tokens\$\s*:?\s*(\d+)', clean_text)
        if match:
            return int(match.group(1)), 0.9, "TAG_MATCH"

        # Strategy 2: Semantic patterns
        semantic_patterns = [
            (r'estimated\s*tokens?\s*:?\s*(\d+)', 0.85),
            (r'token\s*(?:budget|estimate|prediction)\s*:?\s*(\d+)', 0.85),
            (r'budget\s*:?\s*(\d+)\s*tokens?', 0.8),
            (r'requires?\s*(?:approximately\s*)?(\d+)\s*tokens?', 0.8),
            (r'needs?\s*(?:around\s*)?(\d+)\s*tokens?', 0.75),
            (r'around\s*(\d+)\s*tokens?', 0.7),
            (r'approximately\s*(\d+)\s*tokens?', 0.7),
            (r'(\d+)\s*tokens?\s*(?:should|would|will)', 0.65),
        ]

        for pattern, conf in semantic_patterns:
            match = re.search(pattern, clean_text)
            if match:
                value = int(match.group(1))
                if 50 <= value <= 5000:  # Sanity check
                    return value, conf, "SEMANTIC_MATCH"

        # Strategy 3: Last reasonable number in text
        all_numbers = re.findall(r'\d+', clean_text)
        if all_numbers:
            candidates = [int(n) for n in all_numbers if 100 <= int(n) <= 4000]
            if candidates:
                return candidates[-1], 0.5, "HEURISTIC_NUMBER"

        # Strategy 4: Use fallback prediction
        if fallback_prediction:
            return fallback_prediction, 0.4, "LEARNER_FALLBACK"

        # Strategy 5: Dynamic fallback based on text length
        fallback = min(max(len(text) * 2, 300), 1000)
        return fallback, 0.2, "DYNAMIC_FALLBACK"

class RobustValidator:
    """
    Multi-stage answer validation system.
    """

    @staticmethod
    def extract_answer(text: str) -> Optional[str]:
        """Extract final answer from solution text."""
        if not text:
            return None

        # Try \boxed{} first (handles nested braces)
        boxed_patterns = [
            r'\\boxed\{([^{}]*(?:\{[^{}]*\}[^{}]*)*)\}',
            r'\\boxed\{(.*?)\}',
        ]

        for pattern in boxed_patterns:
            matches = re.findall(pattern, text)
            if matches:
                return matches[-1].strip()

        # Try text-based patterns
        text_patterns = [
            r'(?:final answer|answer is)[:\s]*([^\.\n]+)',
            r'(?:therefore|thus|hence)[,\s]*(?:the answer is)?[:\s]*([^\.\n]+)',
        ]

        for pattern in text_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).strip()

        return None

    @staticmethod
    def normalize(text: str) -> str:
        """Normalize mathematical expression for comparison."""
        if not text:
            return ""

        # Remove whitespace
        text = re.sub(r'\s+', '', text)

        # Convert fractions: \frac{a}{b} -> (a)/(b)
        text = re.sub(r'\\(?:d|t)?frac\{([^}]*)\}\{([^}]*)\}', r'(\1)/(\2)', text)

        # Remove formatting commands
        text = re.sub(r'\\(?:text|mathbf|mathrm|mathit|bf|it|rm)\{([^}]*)\}', r'\1', text)

        # Remove LaTeX commands - FIXED REGEX
        text = re.sub(r'\\(?:left|right|cdot|times|div|pm|mp)', '', text)

        # Clean symbols
        for char in ['$', '\\', '{', '}', ' ']:
            text = text.replace(char, '')

        # Normalize operators
        text = text.replace('×', '*').replace('÷', '/').replace('**', '^')

        return text.lower().strip()

    @staticmethod
    def check_numeric_equivalence(ans1: str, ans2: str) -> bool:
        """Check if two answers are numerically equivalent."""
        try:
            # Try to evaluate as expressions
            val1 = eval(ans1.replace('^', '**'))
            val2 = eval(ans2.replace('^', '**'))
            return abs(float(val1) - float(val2)) < 1e-9
        except:
            return False

    @classmethod
    def validate(cls, system, problem_text: str, worker_output: str,
                ground_truth: str) -> Tuple[bool, str, str]:
        """
        Validate worker's answer against ground truth.

        Returns: (is_correct, model_answer, true_answer)
        """
        model_ans = cls.extract_answer(worker_output)
        true_ans = cls.extract_answer(ground_truth)

        # Handle missing answers
        if not true_ans:
            return (True, model_ans, "UNKNOWN") if model_ans else (False, "No Answer", "UNKNOWN")

        if not model_ans:
            return False, "No Answer", true_ans
        m_norm = cls.normalize(model_ans)
        t_norm = cls.normalize(true_ans)

        # Stage 1: Exact match
        if m_norm == t_norm:
            return True, model_ans, true_ans

        # Stage 2: Inclusion match (one contains the other)
        if len(t_norm) > 2:
            if t_norm in m_norm or m_norm in t_norm:
                return True, model_ans, true_ans

        # Stage 3: Numeric equivalence
        if cls.check_numeric_equivalence(m_norm, t_norm):
            return True, model_ans, true_ans

        # Stage 4: LLM Judge (if system available)
        if system is not None:
            try:
                verdict = cls._llm_judge(system, problem_text, true_ans, model_ans)
                return verdict.lower() == "correct", model_ans, true_ans
            except Exception as e:
                pass

        return False, model_ans, true_ans

    @staticmethod
    def _llm_judge(system, problem: str, ground_truth: str, model_output: str) -> str:
        """Use LLM to judge answer correctness."""
        prompt = f"""You are a math teacher grading a student's answer.

Problem: {problem[:500]}

Correct Answer: {ground_truth}

Student's Answer: {model_output}

Task: Determine if the student's final answer is mathematically equivalent to the correct answer.
Consider that answers may be written in different forms but still be equivalent.

Respond with ONLY one word: "Correct" or "Incorrect"
"""
        response, _ = system._generate(
            system.manager_model,
            system.manager_tokenizer,
            prompt,
            max_new_tokens=20
        )

        if "correct" in response.lower():
            if "incorrect" in response.lower():
                return "Incorrect"
            return "Correct"
        return "Incorrect"

class TokenBudgetingAgent:
    """
    Main agent that orchestrates all components for intelligent token budgeting.

    Implements the complete pipeline from the proposal:
    1. Difficulty Assessment
    2. Dual Metric Confidence Assessment
    3. Probabilistic Token Allocation
    4. Sequential Handling
    5. Learning & Optimization
    """

    def __init__(self, manager_model: str = None, worker_model: str = None,
                 config: Config = None):
        self.config = config or Config()

        # Initialize all modules
        self.difficulty_assessor = DifficultyAssessor()
        self.confidence_assessor = DualMetricConfidenceAssessor(self.config.EMA_ALPHA)
        self.token_allocator = ProbabilisticTokenAllocator(self.config)
        self.sequential_handler = SequentialHandler(self.config)
        self.penalty_model = PenaltyRewardModel(self.config)
        self.pattern_learner = OutputPatternLearner()
        self.memory = ReflexionMemory()

        # Load models
        print(" Initializing Token Budgeting Agent")
        print("=" * 60)

        manager_model = manager_model or self.config.MANAGER_MODEL
        worker_model = worker_model or self.config.WORKER_MODEL

        print(f" Loading Manager: {manager_model}")
        self.manager_tokenizer = AutoTokenizer.from_pretrained(
            manager_model, trust_remote_code=True
        )
        self.manager_model = AutoModelForCausalLM.from_pretrained(
            manager_model,
            torch_dtype=self.config.DTYPE,
            device_map=self.config.DEVICE_MAP,
            trust_remote_code=True
        ).eval()

        print(f" Loading Worker: {worker_model}")
        self.worker_tokenizer = AutoTokenizer.from_pretrained(
            worker_model, trust_remote_code=True
        )
        self.worker_model = AutoModelForCausalLM.from_pretrained(
            worker_model,
            torch_dtype=self.config.DTYPE,
            device_map=self.config.DEVICE_MAP,
            trust_remote_code=True
        ).eval()

        print(" Agent initialized successfully!")
        print("=" * 60)

    def _generate(self, model, tokenizer, prompt: str,
                 max_new_tokens: int) -> Tuple[str, int]:
        """Generate text from a model."""
        messages = [{"role": "user", "content": prompt}]
        formatted = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        inputs = tokenizer(formatted, return_tensors="pt").to(model.device)

        with torch.no_grad():
            output = model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id
            )

        generated_ids = output[0, inputs.input_ids.shape[-1]:]
        text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
        tokens_used = len(generated_ids)

        # Cleanup
        del inputs, output
        torch.cuda.empty_cache()
        gc.collect()

        return text, tokens_used

    # ==========================================
    # PHASE 1: ASSESSMENT & PREDICTION
    # ==========================================
    def _assess_problem(self, problem_text: str) -> Tuple[ProblemFeatures, DifficultyLevel, Dict]:
        """
        Phase 1: Assess problem difficulty and extract features.
        """
        # Extract features
        features = self.difficulty_assessor.extract_features(problem_text)

        # Assess difficulty
        difficulty, assessment_confidence, details = self.difficulty_assessor.assess_difficulty(
            problem_text, features
        )

        return features, difficulty, details

    def _get_manager_prediction(self, problem_text: str, features: ProblemFeatures,
                                difficulty: DifficultyLevel) -> Tuple[int, str, ConfidenceScore]:
        """
        Get token prediction from manager model with confidence assessment.
        """
        # Get pattern-based prediction first
        learner_prediction, learner_confidence, learner_method = self.pattern_learner.predict_tokens(
            problem_text, features
        )
        learner_insight = self.pattern_learner.get_insight(features)

        # Get memory context
        memory_context = self.memory.get_context()

        # Get penalty adjustment recommendation
        penalty_adj, penalty_reason = self.penalty_model.get_adjustment_recommendation()

        # Build calibration hint
        calibration_hint = ""
        bias = self.confidence_assessor.global_calibration['bias']
        if abs(bias) > 0.15:
            direction = "OVER" if bias > 0 else "UNDER"
            calibration_hint = f"\n CALIBRATION: You have been {direction}-predicting by ~{abs(bias):.0%}. Adjust accordingly."

        # Build prompt
        prompt = f"""SYSTEM: You are a Token Budget Estimator for math problem solving.
Your task is to predict how many tokens will be needed to solve the given problem.

{calibration_hint}

{memory_context}

--- AUTOMATED ANALYSIS ---
Category: {features.category.upper()}
Difficulty: {difficulty.name}
Complexity Indicators: {learner_insight}
Pattern-Based Estimate: {learner_prediction} tokens (confidence: {learner_confidence:.0%})
Penalty Adjustment: {penalty_adj:+.0%} ({penalty_reason})

--- PROBLEM TO ANALYZE ---
{problem_text}

--- INSTRUCTIONS ---
1. Analyze the problem's mathematical complexity
2. Consider: number of steps, concepts involved, potential edge cases
3. Compare with the pattern-based estimate
4. Provide your calibrated prediction

--- RESPONSE FORMAT ---
[ANALYSIS] <Brief analysis of complexity>
[CONFIDENCE] <low/medium/high/very high>
[TOKENS] <Your token estimate as a single number>
"""

        # Get manager response
        response, _ = self._generate(
            self.manager_model, self.manager_tokenizer, prompt, max_new_tokens=300
        )

        # Parse token prediction
        manager_tokens, parse_confidence, parse_method = TokenParser.extract(
            response, learner_prediction
        )

        # Assess confidence using dual metric system
        confidence = self.confidence_assessor.assess_confidence(
            problem_text, features, response, manager_tokens, learner_prediction
        )

        return manager_tokens, response, confidence

    def _allocate_tokens(self, base_prediction: int, confidence: ConfidenceScore,
                        difficulty: DifficultyLevel, features: ProblemFeatures) -> TokenAllocation:
        """
        Phase 2: Allocate tokens using probabilistic model.
        """
        # Check if we should be conservative due to cascade risk
        if self.sequential_handler.should_apply_conservative_strategy():
            cascade_adj = self.sequential_handler.get_cascade_adjustment()
            base_prediction = int(base_prediction * cascade_adj)

        # Get allocation from probabilistic allocator
        allocation = self.token_allocator.allocate_tokens(
            base_prediction, confidence, difficulty, features
        )

        return allocation

    def _execute_worker(self, problem_text: str, budget: int) -> Tuple[str, int, bool]:
        """
        Execute worker model with given token budget.
        """
        prompt = f"""Problem: {problem_text}

Please solve this problem step-by-step. Show your reasoning clearly.
Put your final answer in \\boxed{{}}.

Solution:
"""
        response, tokens_used = self._generate(
            self.worker_model, self.worker_tokenizer, prompt, max_new_tokens=budget
        )

        hit_limit = tokens_used >= budget - 5  # Small buffer for tokenization

        return response, tokens_used, hit_limit

    def _reflect_on_success(self, problem: str, predicted: int,
                           actual: int, solution: str) -> Tuple[str, float]:
        """Generate reflection on successful execution."""
        accuracy = 1 - abs(predicted - actual) / max(actual, 1)

        prompt = f"""SYSTEM: Analyze why this math problem required {actual} tokens to solve.

PROBLEM: "{problem[:250]}..."
PREDICTED: {predicted} tokens
ACTUAL: {actual} tokens
ACCURACY: {accuracy:.1%}

Explain in 1-2 sentences:
1. What made this problem require {actual} tokens?
2. What features indicated this complexity?
"""

        analysis, _ = self._generate(
            self.manager_model, self.manager_tokenizer, prompt, max_new_tokens=150
        )

        return analysis.strip(), accuracy

    def _reflect_on_failure(self, problem: str, predicted: int, actual: int,
                           failure_reason: str, penalty_info: str) -> Tuple[str, str]:
        """Generate reflection on failed execution."""
        prompt = f"""SYSTEM: Analyze why this prediction failed.

PROBLEM: "{problem[:250]}..."
PREDICTED: {predicted} tokens
ACTUAL USED: {actual} tokens
FAILURE: {failure_reason}
PENALTY: {penalty_info}

Generate:
1. REFLECTION: Why was the prediction wrong? (2-3 sentences)
2. ADJUSTMENT: What should be done differently? (1 sentence)

Format:
REFLECTION: <your reflection>
ADJUSTMENT: <your adjustment recommendation>
"""

        response, _ = self._generate(
            self.manager_model, self.manager_tokenizer, prompt, max_new_tokens=200
        )

        # Parse response
        reflection_match = re.search(r'REFLECTION:\s*(.+?)(?=ADJUSTMENT:|$)', response, re.DOTALL)
        adjustment_match = re.search(r'ADJUSTMENT:\s*(.+)', response, re.DOTALL)

        reflection = reflection_match.group(1).strip() if reflection_match else response.strip()
        adjustment = adjustment_match.group(1).strip() if adjustment_match else ""

        return reflection, adjustment

    # ==========================================
    # MAIN PROCESSING LOOP
    # ==========================================
    def process_problem(self, problem_text: str, ground_truth: str,
                       problem_idx: int) -> ExecutionResult:
        """
        Process a single problem through the complete pipeline.
        """
        start_time = time.time()

        print(f"\n{'='*70}")
        print(f" PROBLEM #{problem_idx}")
        print(f"{'='*70}")

        # ===== PHASE 1: ASSESSMENT =====
        print(f"\n [PHASE 1: ASSESSMENT]")
        features, difficulty, assessment_details = self._assess_problem(problem_text)

        print(f"   Category: {features.category.upper()}")
        print(f"   Difficulty: {difficulty.name}")
        print(f"   Complexity Score: {assessment_details['complexity_score']:.1f}")

        # ===== PHASE 2: PREDICTION & CONFIDENCE =====
        print(f"\n [PHASE 2: PREDICTION & CONFIDENCE]")
        base_prediction, manager_response, confidence = self._get_manager_prediction(
            problem_text, features, difficulty
        )

        print(f"   Base Prediction: {base_prediction} tokens")
        print(f"   Self-Reported Confidence: {confidence.self_reported:.1%}")
        print(f"   Peer Review Confidence: {confidence.peer_review:.1%}")
        print(f"   Combined Confidence: {confidence.combined:.1%}")

        # ===== PHASE 3: ALLOCATION =====
        print(f"\n [PHASE 3: TOKEN ALLOCATION]")
        allocation = self._allocate_tokens(base_prediction, confidence, difficulty, features)

        print(f"   {allocation.allocation_reason}")

        # ===== PHASE 4: EXECUTION WITH RETRIES =====
        print(f"\n [PHASE 4: EXECUTION]")

        attempt = 0
        budget = allocation.final_budget
        result = ExecutionResult(
            predicted_tokens=base_prediction,
            confidence=confidence
        )

        while attempt <= self.config.MAX_RETRIES:
            print(f"\n   Attempt {attempt + 1}/{self.config.MAX_RETRIES + 1} | Budget: {budget} tokens")

            # Execute worker
            solution, tokens_used, hit_limit = self._execute_worker(problem_text, budget)

            # Validate answer
            is_correct, model_ans, true_ans = RobustValidator.validate(
                self, problem_text, solution, ground_truth
            )

            # Calculate penalty
            penalty, penalty_type, penalty_details = self.penalty_model.calculate_penalty(
                base_prediction, tokens_used, budget, is_correct, hit_limit
            )

            print(f"   Tokens Used: {tokens_used} | Hit Limit: {hit_limit}")
            print(f"   Answer: {model_ans[:50] if model_ans else 'None'}...")

            print(f"   Expected: {true_ans[:50] if true_ans else 'None'}...")
            print(f"   Penalty: {penalty:.2f} ({penalty_type})")

            if is_correct:
                # ===== SUCCESS PATH =====
                print(f"\n SUCCESS!")

                # Update result
                result.success = True
                result.actual_tokens = tokens_used
                result.budget_used = budget
                result.hit_limit = hit_limit
                result.retries = attempt
                result.model_answer = model_ans
                result.true_answer = true_ans
                result.penalty = penalty
                result.penalty_type = penalty_type
                result.execution_time = time.time() - start_time

                # ===== PHASE 5: LEARNING =====
                self._update_on_success(
                    problem_text, solution, features, difficulty,
                    base_prediction, tokens_used, budget, confidence,
                    hit_limit, penalty, penalty_type
                )

                # Print efficiency stats
                self._print_success_stats(result, budget)

                return result

            # ===== FAILURE PATH =====
            fail_reason = "TIMEOUT" if hit_limit else "WRONG_ANSWER"
            print(f"\n FAILED: {fail_reason}")

            # Update learning modules with failure
            self._update_on_failure(
                problem_text, features, base_prediction, tokens_used,
                hit_limit, penalty, penalty_type, penalty_details
            )

            # Check retry strategy
            attempt += 1
            should_retry, new_budget, retry_reason = self.sequential_handler.get_retry_strategy(
                attempt, hit_limit, budget
            )

            if not should_retry:
                print(f" {retry_reason}")
                break

            print(f"\n {retry_reason}")
            budget = new_budget

        # ===== FINAL FAILURE =====
        print(f"\n FINAL FAILURE after {attempt} attempts")

        # Update result
        result.success = False
        result.actual_tokens = tokens_used
        result.budget_used = budget
        result.hit_limit = hit_limit
        result.retries = attempt
        result.model_answer = model_ans
        result.true_answer = true_ans
        result.penalty = penalty
        result.penalty_type = penalty_type
        result.execution_time = time.time() - start_time

        # Learn from failure
        self.pattern_learner.analyze_output(problem_text, solution, features, tokens_used)

        # Update sequential handler
        self.sequential_handler.update_state(result)

        return result

    def _update_on_success(self, problem_text: str, solution: str,
                          features: ProblemFeatures, difficulty: DifficultyLevel,
                          predicted: int, actual: int, budget: int,
                          confidence: ConfidenceScore, hit_limit: bool,
                          penalty: float, penalty_type: str):
        """Update all learning modules after successful execution."""

        # 1. Update difficulty assessor
        self.difficulty_assessor.update_history(features, difficulty, actual, True)

        # 2. Update confidence assessor
        self.confidence_assessor.update_from_result(features, predicted, actual, True)

        # 3. Update token allocator
        allocation = TokenAllocation(
            base_prediction=predicted,
            final_budget=budget,
            difficulty_level=difficulty
        )
        self.token_allocator.update_from_result(
            allocation, actual, True, hit_limit, features.category
        )

        # 4. Update pattern learner
        self.pattern_learner.analyze_output(problem_text, solution, features, actual)

        # 5. Generate and store reflection
        analysis, accuracy = self._reflect_on_success(problem_text, predicted, actual, solution)

        # 6. Update memory
        self.memory.add_prediction_feedback(predicted, actual, features.category, "SUCCESS")
        self.memory.add_success(problem_text, actual, analysis, accuracy, features.category)

        # 7. Update sequential handler
        result = ExecutionResult(success=True, actual_tokens=actual, budget_used=budget)
        self.sequential_handler.update_state(result)

    def _update_on_failure(self, problem_text: str, features: ProblemFeatures,
                          predicted: int, actual: int, hit_limit: bool,
                          penalty: float, penalty_type: str, penalty_details: Dict):
        """Update learning modules after failed execution."""

        # 1. Update confidence assessor
        self.confidence_assessor.update_from_result(features, predicted, actual, False)

        # 2. Update memory with feedback
        fail_reason = "TIMEOUT" if hit_limit else "WRONG_ANSWER"
        self.memory.add_prediction_feedback(predicted, actual, features.category, fail_reason)

        # 3. Generate and store reflection
        penalty_info = f"{penalty_type}: {penalty:.2f}"
        reflection, adjustment = self._reflect_on_failure(
            problem_text, predicted, actual, fail_reason, penalty_info
        )

        print(f" Reflection: {reflection[:100]}...")
        if adjustment:
            print(f" Adjustment: {adjustment[:80]}...")

        # 4. Add to memory
        self.memory.add_reflection(problem_text, reflection, penalty_info, features.category)

    def _print_success_stats(self, result: ExecutionResult, budget: int):
        """Print statistics after successful execution."""
        accuracy = 1 - abs(result.predicted_tokens - result.actual_tokens) / max(result.actual_tokens, 1)
        waste = max(0, budget - result.actual_tokens)
        efficiency = self.penalty_model.get_efficiency_score()

        print(f"\n [STATISTICS]")
        print(f"   Prediction Accuracy: {accuracy:.1%}")
        print(f"   Token Waste: {waste} ({waste/budget:.1%} of budget)")
        print(f"   Overall Efficiency: {efficiency:.1%}")
        print(f"   Execution Time: {result.execution_time:.2f}s")

    # ==========================================
    # SESSION MANAGEMENT
    # ==========================================
    def run_session(self, problems: List[Dict], start_idx: int = 0) -> List[ExecutionResult]:
        """
        Run a complete session on multiple problems.
        """
        results = []

        print(f"\n{'='*70}")
        print(f" STARTING SESSION: {len(problems)} problems")
        print(f"{'='*70}")

        for i, item in enumerate(problems):
            problem_idx = start_idx + i

            try:
                result = self.process_problem(
                    item["problem"],
                    item["solution"],
                    problem_idx
                )
                results.append(result)

            except Exception as e:
                print(f"\n Error processing problem #{problem_idx}: {e}")
                import traceback
                traceback.print_exc()

                # Create failure result
                results.append(ExecutionResult(
                    success=False,
                    penalty=2.0,
                    penalty_type="ERROR"
                ))

            # Periodic checkpoint
            if (i + 1) % 10 == 0:
                self._print_checkpoint(results, i + 1, len(problems))

        # Final summary
        self.print_session_summary(results)

        return results

    def _print_checkpoint(self, results: List[ExecutionResult],
                         completed: int, total: int):
        """Print checkpoint summary."""
        successes = sum(1 for r in results if r.success)
        success_rate = successes / len(results) if results else 0

        seq_state = self.sequential_handler.get_state_summary()
        efficiency = self.penalty_model.get_efficiency_score()

        print(f"\n{'─'*60}")
        print(f" CHECKPOINT: {completed}/{total} problems")
        print(f"   Success Rate: {successes}/{completed} ({success_rate:.1%})")
        print(f"   Cascade Risk: {seq_state['cascade_risk']}")
        print(f"   Efficiency Score: {efficiency:.1%}")
        print(f"   Tokens Saved: {seq_state['tokens_saved']}")
        print(f"{'─'*60}")

    def print_session_summary(self, results: List[ExecutionResult]):
        """Print comprehensive session summary."""
        print(f"\n{'='*70}")
        print(f" SESSION SUMMARY")
        print(f"{'='*70}")

        if not results:
            print("No results to summarize.")
            return

        # Overall performance
        successes = [r for r in results if r.success]
        failures = [r for r in results if not r.success]

        print(f"\n OVERALL PERFORMANCE")
        print(f"   Total Problems: {len(results)}")
        print(f"   Success Rate: {len(successes)}/{len(results)} ({len(successes)/len(results):.1%})")
        print(f"   Total Retries: {sum(r.retries for r in results)}")
        print(f"   Total Time: {sum(r.execution_time for r in results):.1f}s")

        # Success metrics
        if successes:
            print(f"\n SUCCESS METRICS")
            accuracies = [1 - abs(r.predicted_tokens - r.actual_tokens) / max(r.actual_tokens, 1)
                         for r in successes]
            print(f"   Avg Prediction Accuracy: {np.mean(accuracies):.1%}")
            print(f"   Avg Tokens Used: {np.mean([r.actual_tokens for r in successes]):.0f}")
            print(f"   Avg Budget Allocated: {np.mean([r.budget_used for r in successes]):.0f}")

            # Efficiency
            total_used = sum(r.actual_tokens for r in successes)
            total_budget = sum(r.budget_used for r in successes)
            print(f"   Token Efficiency: {total_used/total_budget:.1%}")

        # Failure analysis
        if failures:
            print(f"\n FAILURE ANALYSIS")
            print(f"   Total Failures: {len(failures)}")

            timeout_count = sum(1 for r in failures if r.hit_limit)
            wrong_count = len(failures) - timeout_count
            print(f"   Timeouts: {timeout_count}")
            print(f"   Wrong Answers: {wrong_count}")

        # Penalty model summary
        penalty_summary = self.penalty_model.get_summary()
        print(f"\n⚖️ PENALTY MODEL")
        print(f"   Efficiency Score: {penalty_summary['efficiency_score']}")
        print(f"   Cumulative Waste: {penalty_summary['cumulative_waste']} tokens")
        print(f"   Cumulative Shortage: {penalty_summary['cumulative_shortage']} tokens")
        print(f"   Recommendation: {penalty_summary['adjustment_recommendation']}")

        # Sequential handler summary
        seq_summary = self.sequential_handler.get_state_summary()
        print(f"\n SEQUENTIAL HANDLING")
        print(f"   Cascade Risk: {seq_summary['cascade_risk']}")
        print(f"   Tokens Saved: {seq_summary['tokens_saved']}")
        print(f"   Tokens Wasted: {seq_summary['tokens_wasted']}")

        # Learning summary
        learning_stats = self.pattern_learner.get_learning_stats()
        memory_stats = self.memory.get_stats()

        print(f"\n LEARNING STATUS")
        print(f"   Patterns Learned: {learning_stats['total_patterns']}")
        print(f"   Categories: {', '.join(learning_stats['categories_learned']) or 'None'}")
        print(f"   Success Exemplars: {memory_stats['success_count']}")
        print(f"   Failure Reflections: {memory_stats['reflection_count']}")

        # Confidence calibration
        cal = self.confidence_assessor.global_calibration
        print(f"\n CONFIDENCE CALIBRATION")
        print(f"   Global Bias: {cal['bias']:+.1%}")
        print(f"   Global Accuracy: {cal['accuracy']:.1%}")


# ==========================================
# MAIN EXECUTION
# ==========================================
def main():
    """Main entry point for the Token Budgeting System."""

    try:
        # Initialize configuration
        config = Config()

        # Initialize agent
        agent = TokenBudgetingAgent(config=config)

        # Load dataset
        print(f"\n Loading dataset: {config.DATASET_NAME}")
        dataset = load_dataset(config.DATASET_NAME, split="train")

        # Select problems
        indices = np.arange(config.START_INDEX, config.START_INDEX + config.NUM_PROBLEMS)
        problems = dataset.select(indices)

        print(f"   Selected problems {config.START_INDEX} to {config.START_INDEX + config.NUM_PROBLEMS - 1}")

        # Run session
        results = agent.run_session(
            problems=[{"problem": p["problem"], "solution": p["solution"]} for p in problems],
            start_idx=config.START_INDEX
        )

        return results

    except KeyboardInterrupt:
        print("\n\n Session interrupted by user")
        return None

    except Exception as e:
        print(f"\n Fatal error: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = main()