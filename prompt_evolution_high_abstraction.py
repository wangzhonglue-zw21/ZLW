# -*- coding: utf-8 -*-
"""Prompt_evolution_high_abstraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wYeP2qQiyilO_9k4GIsmvaUX8g0Illkh
"""

import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

class SelfReflectiveOptimizer:
    def __init__(self, model_name="Qwen/Qwen2.5-7B-Instruct"):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üöÄ Loading {model_name} on {self.device}...")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map=self.device
        )

        # INITIAL PROMPT (The "Bad" one to start with)
        self.current_prompt = """You are a mathematical hardness analyst. Your goal is to predict the token budget for a solver model.

[CONTEXT INFO]
Problems in this specific problem set typically require between **200 and 2000 tokens** to solve.

[PROBLEM]
{problem_text}

Step 1: **Categorization and Structural Analysis**
First, classify the problem. Identify recurring mathematical forms.

Step 2: **Mental Simulation**
Break down the problem into its main components verbally.

Step 3: **Complexity Assessment**
Rate each parameter (1-10 scale):
- Conceptual Depth
- Computational Effort
- Abstraction Level
- Heuristic Requirements
- Constrainedness

Step 4: **Final Synthesis and Budget Output**
Based on this comprehensive analysis, output the final token budget.

Output your entire thought process followed by:
[TOKENS] <Integer>"""

    def _generate(self, prompt, max_tokens=1500, temperature=0.7):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                repetition_penalty=1.05
            )
        # Decode only the new tokens
        return self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True).strip()

    # --- PHASE 1: PREDICT (Manager) ---
    def predict_tokens(self, problem):
        print("\nüîµ MANAGER IS PREDICTING...")

        # Insert problem into current system prompt
        if "{problem_text}" in self.current_prompt:
            prompt_text = self.current_prompt.replace("{problem_text}", problem)
        else:
            prompt_text = self.current_prompt + f"\n\n[PROBLEM]\n{problem}"

        response = self._generate(prompt_text)

        # Extract the integer prediction
        match = re.search(r"\[TOKENS\]\s*(\d+)", response)
        predicted_val = int(match.group(1)) if match else 0

        # Clean up response for storage (stop at the token tag)
        clean_reasoning = response
        if match:
            clean_reasoning = response[:match.end()]

        return predicted_val, clean_reasoning

    # --- PHASE 2: SOLVE (Worker) ---
    def worker_solve(self, problem):
        print("ü§ñ WORKER IS SOLVING...")

        # Standard solver prompt
        worker_prompt = (
            "You are a math solver. Solve the following problem step-by-step.\n\n"
            f"Problem: {problem}\n\nSolution:"
        )

        solution = self._generate(worker_prompt, max_tokens=2048)

        # Count ACTUAL tokens used (approximate via tokenizer)
        actual_tokens = len(self.tokenizer.encode(solution))

        return actual_tokens, solution

    # --- PHASE 3: REFLECT (Prompt Engineer) ---
    def optimize_prompt(self, problem, manager_reasoning, predicted, worker_solution, actual):
        print("\nüß¨ REFLECTION & OPTIMIZATION TRIGGERED...")

        # 1. Construct the Experiment Result History
        experiment_result = f"""
[EXPERIMENT DATA]
PROBLEM:
{problem}

CURRENT MANAGER PROMPT:
{self.current_prompt}

MANAGER REASONING (FAILED):
{manager_reasoning}

MANAGER PREDICTION: {predicted} tokens

WORKER ACTUAL SOLUTION:
{worker_solution[:1000]}... (truncated)

WORKER ACTUAL COST: {actual} tokens
"""

        # 2. The "Meta-Instruction" (The fixed method from our chat)
        user_instruction = f"""
Consider the experiment data above.
The "Manager" (Predictor) failed significantly. It predicted {predicted}, but actual was {actual}.
Before doing your task, take a close read of the principles of verbal reinforcement learning principles, you should not be citing them in your prompt but try embody the spirit:
**THE 5-POINT PROTOCOL FOR HIGH-ABSTRACTION PROMPTING**
Rigid miniature feature matching such as keyword-matching formulas for token prediction doesn't work very well in verbal reinforcement learning. Instead high level abstractions and summaries/insights works better.

1. **Schema > Keywords:**
   - DON'T: "Count how many times 'summation' appears."
   - DO: "Identify the Solution Archetype (e.g., Constructive Proof, Brute Force, Logical Derivation, Induction)."

2. **Narrative Simulation:**
   - DON'T: "How hard is this?"
   - DO: "Visualize the solution path. What specific transitions (e.g., variable definition, case splitting, algebraic expansion, proof structure) must the solver write?"

3. **Volume Multipliers:**
   - DON'T: "Add 20 tokens for case work."
   - DO: "Does this require branching logic (e.g., 'If n is even/odd')? If so, apply a multiplier to the base estimate based on the narrative complexity."

4. **Semantic Gradients (Feedback):**
   - DON'T: "Wrong. You were off by {abs(predicted - actual)} tokens."
   - DO: "You failed because you underestimated the overhead of a specific structure such as algebraic manipulations. Adjust for this pattern."

5. **Meta-Cognitive Handoff:**
   - DON'T: Manually tweak prompt words.
   - DO: Feed the failure analysis back and ask: 'Rewrite your own System Prompt to prevent this specific error next time.'

**Your Task:**
1. **Analyze:** Why did the prompt overestimate/underestimate? Explain specifically how each component of the original prompt lead to the result.
2. **Improve:** Based on the original prompt, write a NEW System Prompt that will help predict token usage for ANY math problem better. If you feel that certain component of the original prompt is working, please leave them intact. Revise or delete things that are inadequate. Add new insights/philosophy if necessary.
3. **output** Make sure your prompt work properly, it should have similar end structure (Output your entire thought process followed by:[TOKENS] <Integer>) as in the original prompt so that it will force the LLM to output token prediction in correct format.
[CRITICAL CONSTRAINTS]
- STOP immediately after the prompt, make sure not to duplicate output
- Keep {{problem_text}} placeholder
- Output only: ### ANALYSIS and ### NEW SYSTEM PROMPT, the prompt should be design to predict token usage to solve any math problem, not solve the problem.

[OUTPUT FORMAT]
### ANALYSIS
(Single sentence)
### NEW SYSTEM PROMPT
(Full prompt text)
"""

        # 3. Ask Qwen to fix it
        messages = [
            {"role": "system", "content": "You are an Expert Prompt Engineer and LLM Optimizer."},
            {"role": "user", "content": experiment_result + "\n\n" + user_instruction}
        ]
        text_input = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

        # Generate the optimization
        optimization_response = self._generate(text_input, max_tokens=1500)

        return optimization_response

    # --- MAIN CYCLE ---
    def run_optimization_loop(self, problem_text):
        # 1. Predict
        pred_val, pred_text = self.predict_tokens(problem_text)
        print(f"   -> Predicted: {pred_val}")

        # 2. Solve (Ground Truth)
        actual_val, solution_text = self.worker_solve(problem_text)
        print(f"   -> Actual: {actual_val}")

        # 3. Check Deviation
        error = abs(pred_val - actual_val)
        print(f"   -> Deviation: {error}")

        # 4. Reflect if error is high (e.g., > 300 tokens)
        if error > 300:
            print("   -> ‚ö†Ô∏è LARGE ERROR DETECTED. OPTIMIZING PROMPT...")

            optimization_output = self.optimize_prompt(
                problem=problem_text,
                manager_reasoning=pred_text,
                predicted=pred_val,
                worker_solution=solution_text,
                actual=actual_val
            )

            print("="*40)
            print("NEW OPTIMIZED PROMPT CANDIDATE:")
            print(optimization_output)
            print("="*40)

            # Simple extraction to update the prompt for next time
            if "### NEW SYSTEM PROMPT" in optimization_output:
                new_prompt = optimization_output.split("### NEW SYSTEM PROMPT")[1].strip()
                self.current_prompt = new_prompt
                print("‚úÖ PROMPT UPDATED IN MEMORY.")
        else:
            print("‚úÖ Prediction was acceptable.")

# --- EXECUTION ---
if __name__ == "__main__":
    optimizer = SelfReflectiveOptimizer()

    # The Test Problem
    math_problem = r"""
Given a sequence $\{a_n\}$ with each term being a positive number, and satisfying $a_2=5$, $$a_{n+1}=a_{n}^2-2na_n+2 \quad (n \in \mathbb{N}^*).$$
(1) Conjecture the general formula for $\{a_n\}$.
(2) Let $b_n=2^{n-1}$ and $c_n=a_n+b_n$, find the sum of the first $n$ terms of the sequence $\{c_n\}$, denoted as $T_n$.
"""

    optimizer.run_optimization_loop(math_problem)
# --- PHASE 4: VERIFICATION ---
print("\nüîÑ VERIFYING THE FIX...")

# Re-run prediction with the NOW UPDATED system prompt
new_pred, new_reasoning = optimizer.predict_tokens(math_problem)

print("-" * 30)
print(f"üìà New Prediction: {new_pred} tokens")
print("-" * 30)

# Optional: Print the new reasoning to see if it used the list!
print("\n[NEW MANAGER REASONING]")
print(new_reasoning)